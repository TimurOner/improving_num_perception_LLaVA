{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOnkFmkSqPmPmzU9a4Bplhx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"qjab4h3TBzJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8x7Noj-Bj_F"},"outputs":[],"source":["# Useful probing dataset class definitions\n","\n","class LogisticRegressionCLSDataset(Dataset):\n","    def __init__(self, embeddings, labels):\n","\n","        self.embeddings = embeddings\n","        if isinstance(labels, np.ndarray):\n","            self.labels = torch.from_numpy(labels).long()\n","        else:\n","            self.labels = labels.long()\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        x = self.embeddings[idx]\n","        y = self.labels[idx].item()\n","        return x, y\n","\n","class LogisticRegressionPatchDataset(Dataset):\n","    def __init__(self, embeddings, labels, flatten=False):\n","        self.embeddings = embeddings\n","        if isinstance(labels, np.ndarray):\n","            self.labels = torch.from_numpy(labels).long()\n","        else:\n","            self.labels = labels.long()\n","        self.flatten = flatten\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        x = self.embeddings[idx]\n","        if self.flatten:\n","            x = x.view(-1)\n","        y = self.labels[idx].item()\n","        return x, y\n"]},{"cell_type":"code","source":["# The Linear Probe Classes\n","class NumerosityLinearProbeCLS(nn.Module):\n","    def __init__(self, input_dim=4096, num_classes=10):\n","        super(NumerosityLinearProbeCLS, self).__init__()\n","        self.classifier = nn.Linear(input_dim, num_classes)\n","\n","    def forward(self, x):\n","        logits = self.classifier(x)\n","        probs = F.softmax(logits, dim=-1)\n","        return logits, probs\n","class NumerosityLinearProbeFlattenPatch(nn.Module):\n","    def __init__(self, input_tokens=576, input_dim=4096, num_classes=10):\n","        super(NumerosityLinearProbeFlattenPatch, self).__init__()\n","        self.input_tokens = input_tokens\n","        self.input_dim = input_dim\n","        self.classifier = nn.Linear(input_tokens * input_dim, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, 577, 4096)\n","        x_flat = x.view(x.size(0), -1)  # Flatten to (batch_size, 577 * 4096)\n","        logits = self.classifier(x_flat)\n","        probs = F.softmax(logits, dim=-1)\n","        return logits, probs"],"metadata":{"id":"AcaAveAzBykm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_stratified_datasets(embeddings, labels, patch=False, flatten=False):\n","    if hasattr(embeddings, 'cpu'):\n","        embeddings_np = embeddings.cpu().numpy()\n","    else:\n","        embeddings_np = embeddings\n","\n","    if hasattr(labels, 'cpu'):\n","        labels_np = labels.cpu().numpy()\n","    else:\n","        labels_np = labels\n","\n","    X_trainval, X_test, y_trainval, y_test = train_test_split(\n","        embeddings_np,\n","        labels_np,\n","        test_size=0.2,\n","        stratify=labels_np,\n","        random_state=42\n","    )\n","\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_trainval,\n","        y_trainval,\n","        test_size=0.25,\n","        stratify=y_trainval,\n","        random_state=42\n","    )\n","\n","    X_train = torch.tensor(X_train, dtype=torch.float32)\n","    X_val   = torch.tensor(X_val, dtype=torch.float32)\n","    X_test  = torch.tensor(X_test, dtype=torch.float32)\n","    y_train = torch.tensor(y_train, dtype=torch.long)\n","    y_val   = torch.tensor(y_val, dtype=torch.long)\n","    y_test  = torch.tensor(y_test, dtype=torch.long)\n","\n","    if patch:\n","        train_dataset = LogisticRegressionPatchDataset(X_train, y_train, flatten=flatten)\n","        val_dataset   = LogisticRegressionPatchDataset(X_val, y_val, flatten=flatten)\n","        test_dataset  = LogisticRegressionPatchDataset(X_test, y_test, flatten=flatten)\n","    else:\n","        train_dataset = LogisticRegressionCLSDataset(X_train, y_train)\n","        val_dataset   = LogisticRegressionCLSDataset(X_val, y_val)\n","        test_dataset  = LogisticRegressionCLSDataset(X_test, y_test)\n","\n","    print(f\"Dataset splits:\")\n","    print(f\"  Train: {len(train_dataset)} samples\")\n","    print(f\"  Val:   {len(val_dataset)} samples\")\n","    print(f\"  Test:  {len(test_dataset)} samples\")\n","    print(f\"  Total: {len(train_dataset) + len(val_dataset) + len(test_dataset)} samples\")\n","\n","    print(f\"\\nClass distribution:\")\n","    unique_train, counts_train = np.unique(y_train, return_counts=True)\n","    unique_val, counts_val     = np.unique(y_val, return_counts=True)\n","    unique_test, counts_test   = np.unique(y_test, return_counts=True)\n","\n","    print(\"Train:\", dict(zip(unique_train, counts_train)))\n","    print(\"Val:  \", dict(zip(unique_val, counts_val)))\n","    print(\"Test: \", dict(zip(unique_test, counts_test)))\n","\n","    return {\n","        'train_dataset': train_dataset,\n","        'val_dataset': val_dataset,\n","        'test_dataset': test_dataset,\n","        'train_embeddings': X_train,\n","        'val_embeddings': X_val,\n","        'test_embeddings': X_test,\n","        'train_labels': y_train,\n","        'val_labels': y_val,\n","        'test_labels': y_test\n","    }\n","\n","def simple_train_model(model, train_loader, val_loader, optimizer, scheduler, device,\n","                       max_epochs=50, patience=10):\n","\n","    criterion = nn.CrossEntropyLoss()\n","    best_val_loss = float('inf')\n","    epochs_no_improve = 0\n","\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(max_epochs):\n","        # Training phase\n","        model.train()\n","        train_loss = 0.0\n","        train_total = 0\n","\n","        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n","            x_batch = x_batch.to(device, dtype=torch.float32, non_blocking=True)\n","            y_batch = y_batch.to(device, dtype=torch.long, non_blocking=True) - 1\n","\n","            optimizer.zero_grad()\n","\n","            try:\n","                logits, _ = model(x_batch)\n","                loss = criterion(logits, y_batch)\n","\n","                if torch.isnan(loss) or torch.isinf(loss):\n","                    continue\n","\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                optimizer.step()\n","\n","                train_loss += loss.item() * x_batch.size(0)\n","                train_total += x_batch.size(0)\n","\n","            except RuntimeError as e:\n","                print(f\"⚠️ RuntimeError in training batch {batch_idx}: {e}\")\n","                continue\n","\n","        if train_total > 0:\n","            train_loss = train_loss / train_total\n","        else:\n","            break\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","        val_total = 0\n","\n","        with torch.no_grad():\n","            for batch_idx, (x_batch, y_batch) in enumerate(val_loader):\n","                try:\n","                    x_batch = x_batch.to(device, dtype=torch.float32, non_blocking=True)\n","                    y_batch = y_batch.to(device, dtype=torch.long, non_blocking=True) - 1\n","\n","                    logits, _ = model(x_batch)\n","                    loss = criterion(logits, y_batch)\n","\n","                    if torch.isnan(loss) or torch.isinf(loss):\n","                        continue\n","\n","                    val_loss += loss.item() * x_batch.size(0)\n","                    val_total += x_batch.size(0)\n","\n","                except RuntimeError as e:\n","                    print(f\"⚠️ RuntimeError in validation batch {batch_idx}: {e}\")\n","                    continue\n","\n","        if val_total > 0:\n","            val_loss = val_loss / val_total\n","        else:\n","            val_loss = float('inf')\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","\n","        print(f\"Epoch {epoch+1:2d}/{max_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","\n","        if not (torch.isnan(torch.tensor(val_loss)) or torch.isinf(torch.tensor(val_loss))):\n","            scheduler.step(val_loss)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            epochs_no_improve = 0\n","            torch.save(model.state_dict(), 'best_model.pt')\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve >= patience:\n","            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n","            break\n","\n","    try:\n","        model.load_state_dict(torch.load('best_model.pt', map_location=device))\n","    except FileNotFoundError:\n","        print(\"⚠️ Best model file not found. Returning last model state.\")\n","\n","    return model, train_losses, val_losses"],"metadata":{"id":"U4yq_10RE1ma"},"execution_count":null,"outputs":[]}]}
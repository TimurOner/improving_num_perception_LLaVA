# -*- coding: utf-8 -*-
"""modality_vis_functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RoWOdq_yeUTZ8RV-lywy06siri65J5Gy
"""

# Built-in libraries
import os
import json
import random
import re
from typing import Dict, List, Optional, Tuple, Union


# Scientific computing
import numpy as np
import pandas as pd
from scipy.stats import zscore, linregress
from scipy.spatial import ConvexHull

# PyTorch
import torch
import torch.nn.functional as F
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader,random_split, Subset
# Visualization
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib.lines import Line2D
import seaborn as sns
from matplotlib.pyplot import Normalize
import matplotlib.patches as patches

# Machine Learning / Dimensionality Reduction
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
from sklearn.preprocessing import normalize, StandardScaler
from sklearn.metrics import confusion_matrix, mean_absolute_error, accuracy_score,ConfusionMatrixDisplay
from sklearn.metrics.pairwise import cosine_distances
from sklearn.metrics.pairwise import cosine_similarity
# Transformers / Hugging Face
from transformers import (
    AutoProcessor,
    AutoModel,
    AutoTokenizer,
    CLIPModel,
    CLIPProcessor,
    CLIPImageProcessor,
    BitsAndBytesConfig,
    AutoModelForVision2Seq,
    pipeline,
    Trainer,
    TrainingArguments,
    TrainerCallback,
    TrainerState,
    TrainerControl,
    LlavaForConditionalGeneration
)

# PEFT Imports
from peft import PeftModel, PeftConfig, get_peft_model,LoraConfig,TaskType


# UMAP
import umap

# Image processing
from PIL import Image

# SafeTensors
from safetensors.torch import load_file, save_file





#

def load_adapter_weights(model, adapter_weight_path,merge=False):


    # Loading PEFT keys
    safetensors_path = (
        os.path.join(adapter_weight_path, "adapter_model.safetensors")
        if os.path.isdir(adapter_weight_path)
        else adapter_weight_path
    )
    if not os.path.exists(safetensors_path):
        raise FileNotFoundError(f"Adapter weights not found: {safetensors_path}")

    # Load weights
    adapter_weights = load_file(safetensors_path)

    # Load into model
    missing, unexpected = model.load_state_dict(adapter_weights, strict=False)

    print(f"✓ Loaded adapter weights from {safetensors_path}")
    if missing:
        print(f"✗ Missing keys: {missing[:5]}{'...' if len(missing) > 5 else ''}")
    if unexpected:
        print(f"⚠ Unexpected keys: {unexpected[:5]}{'...' if len(unexpected) > 5 else ''}")
    if merge:
        model.merge_and_unload()
        print("✓ Model merged and unloaded")

    return model



####------########------########------########------########------########------########------########------####
#  Embedder functions
####------########------########------########------########------########------########------########------####

# These functions perform the embedding according to the vision and text envoder of LLaVA 1.5 7B. Note that you need to wrap the LLaVA model into Huggingface Transformers pipeline object before being able to pass thgem to these functions,
# An example pipeline creation call is as below:

# example_pipe = pipeline(
#     task="image-to-text",
#     model=finetuned_model_exp3,
#     tokenizer=tokenizer,
#     image_processor=image_processor,
# )

# You need to loadand pass the tokenizer and image_embedder that produce compatible embeddings with LLaVA 1.5 7B.
# You can load these componets by running the code below:
#
# model_name = "llava-hf/llava-1.5-7b-hf"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# image_processor = AutoProcessor.from_pretrained(model_name).image_processo


def prepare_model_for_attention(pipe_object_llava):
    for layer in pipe_object_llava.model.language_model.layers:
        if hasattr(layer, 'self_attn'):
            layer.self_attn.attn_implementation = "eager"

    pipe_object_llava.model.language_model.config._attn_implementation = "eager"
    pipe_object_llava.model.language_model.config.output_attentions = True



def get_all_visual_embeddings(image_PIL, pipe_object_llava):


    img_tensor = torch.tensor(pipe_object_llava.image_processor(image_PIL)['pixel_values'][0]).unsqueeze(0).to(pipe_object_llava.device)

    with torch.no_grad():

        vision_output = pipe_object_llava.model.vision_tower(img_tensor)
        unproj_all_embeddings = vision_output.last_hidden_state  # [1, 577, 1024]


        proj_all_embeddings = pipe_object_llava.model.multi_modal_projector(unproj_all_embeddings)  # [1, 577, 4096]


        vision_embeds = proj_all_embeddings  # [1, 577, 4096]
        dummy_text = pipe_object_llava.tokenizer("\nUSER: How many objects are in this photo? Answer only with a number between 1 and 10. \nASSISTANT:", return_tensors="pt").to(pipe_object_llava.device)
        inputs_embeds = pipe_object_llava.model.language_model.embed_tokens(dummy_text.input_ids)


        multimodal_embeds = torch.cat([vision_embeds, inputs_embeds], dim=1)
        attention_mask = torch.cat([
            torch.ones((1, vision_embeds.shape[1]), device=pipe_object_llava.device),
            dummy_text.attention_mask
        ], dim=1)

        outputs = pipe_object_llava.model.language_model(
            inputs_embeds=multimodal_embeds,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True
        )

        # Get attended embeddings for all 577 visual tokens
        attended_all_embeddings = outputs.hidden_states[-1][:, :vision_embeds.shape[1], :]

    return (
        np.array(attended_all_embeddings.squeeze(0).cpu()),
        np.array(unproj_all_embeddings.squeeze(0).cpu()),
        np.array(proj_all_embeddings.squeeze(0).cpu())
    )

def get_all_visual_embeddings_and_attentions(image_PIL, pipe_object_llava, prompt):
    img_tensor = torch.tensor(pipe_object_llava.image_processor(image_PIL)['pixel_values'][0]).unsqueeze(0).to(pipe_object_llava.device)


    prepare_model_for_attention(pipe_object_llava)

    with torch.no_grad():
        vision_output = pipe_object_llava.model.vision_tower(img_tensor)
        unproj_all_embeddings = vision_output.last_hidden_state

        proj_all_embeddings = pipe_object_llava.model.multi_modal_projector(unproj_all_embeddings)

        dummy_text = pipe_object_llava.tokenizer(prompt, return_tensors="pt").to(pipe_object_llava.device)


        inputs_embeds = pipe_object_llava.model.language_model.embed_tokens(dummy_text.input_ids)

        # Print lengths of visual and text tokens
        print(f"Number of visual tokens: {proj_all_embeddings.shape[1]}")
        print(f"Number of text tokens: {inputs_embeds.shape[1]}")

        multimodal_embeds = torch.cat([proj_all_embeddings, inputs_embeds], dim=1)

        attention_mask = torch.cat([
            torch.ones((1, proj_all_embeddings.shape[1]), device=pipe_object_llava.device),
            dummy_text.attention_mask
        ], dim=1)

        outputs = pipe_object_llava.model.language_model(
            inputs_embeds=multimodal_embeds,
            attention_mask=attention_mask,
            output_hidden_states=True,
            attn_implementation="eager",
            output_attentions=True,
            return_dict=True
        )

        attentions = outputs.attentions

        attentions_tensor = torch.stack(attentions, dim=0)

    return (
        unproj_all_embeddings.squeeze(0).cpu().numpy(),
        proj_all_embeddings.squeeze(0).cpu().numpy(),
        attentions_tensor.cpu().numpy()
    )


def get_text_embeddings(prompt, image_PIL, pipe_object_llava, max_length=13):


    text_tokens = pipe_object_llava.tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True
    ).to(pipe_object_llava.device)

    img_tensor = torch.tensor(pipe_object_llava.image_processor(image_PIL)['pixel_values'][0]).unsqueeze(0).to(pipe_object_llava.device)

    with torch.no_grad():
        inputs_embeds = pipe_object_llava.model.language_model.embed_tokens(text_tokens.input_ids)
        text_embeddings_before = inputs_embeds.cpu().numpy()[0, :, :]


        unproj_cls_embedding = pipe_object_llava.model.vision_tower(img_tensor)[1]
        proj_cls_embedding = pipe_object_llava.model.multi_modal_projector(unproj_cls_embedding)
        vision_embeds = proj_cls_embedding.unsqueeze(1)


        multimodal_embeds = torch.cat([vision_embeds, inputs_embeds], dim=1)
        attention_mask = torch.cat([
            torch.ones((1, vision_embeds.shape[1]), device=pipe_object_llava.device),
            text_tokens.attention_mask
        ], dim=1)


        outputs = pipe_object_llava.model.language_model(
            inputs_embeds=multimodal_embeds,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True
        )

        text_embeddings_after = outputs.hidden_states[-1].squeeze(0)[1:]


        if text_embeddings_after.shape[0] < max_length:
            pad_len = max_length - text_embeddings_after.shape[0]
            pad_tensor = torch.zeros((pad_len, text_embeddings_after.shape[1]), device=text_embeddings_after.device)
            text_embeddings_after = torch.cat([text_embeddings_after, pad_tensor], dim=0)

        if text_embeddings_before.shape[0] < max_length:
            pad_len = max_length - text_embeddings_before.shape[0]
            pad_array = np.zeros((pad_len, text_embeddings_before.shape[1]))
            text_embeddings_before = np.concatenate([text_embeddings_before, pad_array], axis=0)

    return (
        np.array(text_embeddings_after.cpu()),
        text_embeddings_before
    )

####------########------########------########------########------########------########------########------####
#  Embedding pre processing and preparation functions
####------########------########------########------########------########------########------########------####

# The process functions mainly do the preprocessing of text and image embeddings before the plotting tasks.

# Ther functionality include:
#  - Pre-reduction using SVD to reduce dimensionality of embeddings so UMAP and tSNE work better.
#  - Normalizing.
#  - Filtering the embeddings according to certain numerosities.
#  - Pooling the text embeddings to get an approximate single vector as representation of the whole sentence.

def reduce_dimension(
    data_tensor: np.ndarray,
    dim: int,
    dim_reduction_method: str = 'pca',
    SEED: int = 42
) -> np.ndarray:

    number_of_points = data_tensor.shape[0]
    full_dim = data_tensor.shape[1]

    if dim_reduction_method == 'pca':
        reducer = PCA(n_components=dim)
    elif dim_reduction_method == 'svd':
        reducer = TruncatedSVD(n_components=dim)
    elif dim_reduction_method == 'tsne':
        reducer = TSNE(n_components=dim, random_state=SEED)
    elif dim_reduction_method == 'umap':
        reducer = umap.UMAP(n_components=dim, random_state=SEED)
    else:
        raise ValueError(f"Unknown dimensionality reduction method: {dim_reduction_method}")

    reduced_data_tensor = reducer.fit_transform(data_tensor)

    return reduced_data_tensor



def process_text_embeddings(
    text_embeddings,
    numerosity_array,
    max_count,
    numerosities=None,
    dim_red_method='svd',
    pre_reduction=False,
    SEED=42,
    pre_reduction_dim=400,
    normalize='l2',
    pooling_method='mean'
):


    emb_dim = text_embeddings.shape[-1]
    if emb_dim == 4096:
        print('LLaMA text embeddings detected!')
    elif emb_dim == 768:
        print('BERT-like text embeddings detected!')
    elif emb_dim == 1024:
        print('Large text embeddings detected!')
    elif emb_dim == 512:
        print('Standard text embeddings detected!')
    else:
        print(f'Text embeddings with dimension {emb_dim} detected!')


    if text_embeddings.ndim == 3:
        print("Processing token-level embeddings...")
        print("Excluding first and last token embeddings...")
        text_embeddings = text_embeddings[:, 1:-1, :]


        if pooling_method == 'mean':
            text_embeddings = np.mean(text_embeddings, axis=1)
            print("Applied mean pooling across tokens")
        elif pooling_method == 'max':
            text_embeddings = np.max(text_embeddings, axis=1)
            print("Applied max pooling across tokens")
        elif pooling_method == 'sum':
            text_embeddings = np.sum(text_embeddings, axis=1)
            print("Applied sum pooling across tokens")
        elif pooling_method == 'first':
            text_embeddings = text_embeddings[:, 0, :]
            print("Used first token embedding")
        elif pooling_method == 'last':
            text_embeddings = text_embeddings[:, -1, :]
            print("Used last token embedding")
        else:
            raise ValueError(f"Unknown pooling method: {pooling_method}")

    elif text_embeddings.ndim == 2:
        print("Assuming input is already pooled (no token dimension).")
    else:
        raise ValueError("Unsupported shape for text_embeddings. Expected 2D or 3D array.")


    mask = np.ones(len(numerosity_array), dtype=bool)
    if numerosities is not None:
        mask &= np.isin(numerosity_array, numerosities)
        print(f"Filtered for numerosities: {numerosities}")

    text_embeddings_numpy = text_embeddings[mask]
    numerosity_array_filtered = np.array(numerosity_array)[mask]

    print(f"Samples after filtering: {len(text_embeddings_numpy)}")


    np.random.seed(SEED)
    indices = np.random.permutation(len(text_embeddings_numpy))
    text_embeddings_numpy = text_embeddings_numpy[indices]
    numerosity_array_filtered = numerosity_array_filtered[indices]

    max_count = min(max_count, len(text_embeddings_numpy))
    text_embeddings_numpy = text_embeddings_numpy[:max_count]
    numerosity_array_text = numerosity_array_filtered[:max_count]

    print(f"Text embeddings shape after processing: {text_embeddings_numpy.shape}")


    if normalize:
        if normalize == 'l2':
            norms = np.linalg.norm(text_embeddings_numpy, axis=1, keepdims=True)
            text_embeddings_numpy = text_embeddings_numpy / np.clip(norms, a_min=1e-8, a_max=None)
            print("Applied L2 normalization")
        elif normalize == 'standard':
            mean_vals = np.mean(text_embeddings_numpy, axis=0)
            std_vals = np.std(text_embeddings_numpy, axis=0)
            text_embeddings_numpy = (text_embeddings_numpy - mean_vals) / np.clip(std_vals, a_min=1e-8, a_max=None)
            print("Applied standard normalization")
        elif normalize == 'minmax':
            min_vals = np.min(text_embeddings_numpy, axis=0)
            max_vals = np.max(text_embeddings_numpy, axis=0)
            text_embeddings_numpy = (text_embeddings_numpy - min_vals) / np.clip(max_vals - min_vals, a_min=1e-8, a_max=None)
            print("Applied min-max normalization")
        else:
            print(f"Unknown normalization method: {normalize}")


    if pre_reduction:
        if pre_reduction_dim >= text_embeddings_numpy.shape[1]:
            print(f"Skipping pre-reduction: target dimension {pre_reduction_dim} >= current dimension {text_embeddings_numpy.shape[1]}")
        else:
            svd = TruncatedSVD(n_components=pre_reduction_dim, random_state=SEED)
            print(f"Applying SVD pre-reduction to {pre_reduction_dim} dimensions...")
            text_embeddings_numpy = svd.fit_transform(text_embeddings_numpy)
            print(f"Explained variance ratio: {svd.explained_variance_ratio_.sum():.3f}")


    text_embeddings_reduced = reduce_dimension(
        text_embeddings_numpy, 2, dim_red_method, SEED
    )

    return text_embeddings_reduced, numerosity_array_text

def process_multimodal_embeddings(
    img_embeddings,
    text_embeddings,
    numerosity_array,
    obj_type_array,
    max_count,
    numerosities=None,
    object_types=None,
    dim_red_method='svd',
    pre_reduction=False,
    SEED=42,
    separate_reduction=False,
    pre_reduction_dim=400,
    normalize_embeddings=True,
    pooling_method='mean'
):

    emb_dim = img_embeddings.shape[-1]
    if emb_dim == 4096:
        print('LLaVa embeddings detected!')
    elif emb_dim == 1024:
        print('LLaVa Unprojected embeddings detected!')
    elif emb_dim == 512:
        img_embeddings = img_embeddings[:, 0, :]
        print('CLIP embeddings detected!')
    else:
        raise ValueError("Unsupported embedding dimension")


    mask = np.ones(len(numerosity_array), dtype=bool)
    if numerosities is not None:
        mask &= np.isin(numerosity_array, numerosities)
    if object_types is not None:
        mask &= np.isin(obj_type_array, object_types)

    img_embeddings_numpy = img_embeddings[mask]
    text_embeddings_numpy = text_embeddings[mask]
    numerosity_array_filtered = np.array(numerosity_array)[mask]
    obj_type_array_filtered = np.array(obj_type_array)[mask]


    np.random.seed(SEED)
    indices = np.random.permutation(len(img_embeddings_numpy))
    img_embeddings_numpy = img_embeddings_numpy[indices]
    text_embeddings_numpy = text_embeddings_numpy[indices]
    numerosity_array_filtered = numerosity_array_filtered[indices]
    obj_type_array_filtered = obj_type_array_filtered[indices]


    max_count = min(max_count, len(img_embeddings_numpy))
    img_embeddings_numpy = img_embeddings_numpy[:max_count]
    text_embeddings_numpy = text_embeddings_numpy[:max_count]
    numerosity_array_final = numerosity_array_filtered[:max_count]
    obj_type_array_final = obj_type_array_filtered[:max_count]

    img_embeddings_numpy = img_embeddings_numpy.reshape(-1, emb_dim)


    if text_embeddings_numpy.ndim == 3:
        print("Processing token-level embeddings...")
        print("Excluding first and last token embeddings...")
        text_embeddings_numpy = text_embeddings_numpy[:, 1:-1, :]


        if pooling_method == 'mean':
            text_embeddings_numpy = np.mean(text_embeddings_numpy, axis=1)
            print("Applied mean pooling across tokens")
        elif pooling_method == 'max':
            text_embeddings_numpy = np.max(text_embeddings_numpy, axis=1)
            print("Applied max pooling across tokens")
        elif pooling_method == 'sum':
            text_embeddings_numpy = np.sum(text_embeddings_numpy, axis=1)
            print("Applied sum pooling across tokens")
        elif pooling_method == 'first':
            text_embeddings_numpy = text_embeddings_numpy[:, 0, :]
            print("Used first token embedding")
        elif pooling_method == 'last':
            text_embeddings_numpy = text_embeddings_numpy[:, -1, :]
            print("Used last token embedding")
        else:
            raise ValueError(f"Unknown pooling method: {pooling_method}")

    elif text_embeddings_numpy.ndim == 2:
        print("Assuming input is already pooled (no token dimension).")
    else:
        raise ValueError("Unsupported shape for text_embeddings. Expected 2D or 3D array.")

    print(f"Image embeddings shape: {img_embeddings_numpy.shape}")
    print(f"Text embeddings shape: {text_embeddings_numpy.shape}")


    assert img_embeddings_numpy.shape[0] == text_embeddings_numpy.shape[0], \
        f"Mismatch in number of samples: img={img_embeddings_numpy.shape[0]}, text={text_embeddings_numpy.shape[0]}"


    print(f"Image embeddings mean L2 norm before normalization: {np.mean(np.linalg.norm(img_embeddings_numpy, axis=1)):.3f}")
    print(f"Text embeddings mean L2 norm before normalization: {np.mean(np.linalg.norm(text_embeddings_numpy, axis=1)):.3f}")


    if normalize_embeddings:
        print("Normalizing embeddings (L2)...")
        img_embeddings_numpy = normalize(img_embeddings_numpy)
        text_embeddings_numpy = normalize(text_embeddings_numpy)


        print(f"Image embeddings mean L2 norm after normalization: {np.mean(np.linalg.norm(img_embeddings_numpy, axis=1)):.3f}")
        print(f"Text embeddings mean L2 norm after normalization: {np.mean(np.linalg.norm(text_embeddings_numpy, axis=1)):.3f}")


    combined_embeddings = np.concatenate([img_embeddings_numpy, text_embeddings_numpy], axis=0)
    print(f"Combined embeddings shape: {combined_embeddings.shape}")


    if pre_reduction:
        svd = TruncatedSVD(n_components=pre_reduction_dim, random_state=SEED)
        print(f"Applying SVD pre-reduction to {pre_reduction_dim} dimensions...")
        combined_embeddings = svd.fit_transform(combined_embeddings)


    print(f"Applying {dim_red_method.upper()} dimensionality reduction...")
    if dim_red_method.lower() == 'svd':
        reducer = TruncatedSVD(n_components=2, random_state=SEED)
    elif dim_red_method.lower() == 'pca':
        reducer = PCA(n_components=2, random_state=SEED)
    elif dim_red_method.lower() == 'umap':
        reducer = UMAP(n_components=2, random_state=SEED)
    elif dim_red_method.lower() == 'tsne':
        reducer = TSNE(n_components=2, random_state=SEED)
    else:
        raise ValueError("Unsupported dimensionality reduction method. Use 'svd', 'pca', 'umap', or 'tsne'")

    if separate_reduction:
        print("Using separate reduction for each modality...")

        if dim_red_method.lower() == 'svd':
            img_reducer = TruncatedSVD(n_components=2, random_state=SEED)
            text_reducer = TruncatedSVD(n_components=2, random_state=SEED+1)
        elif dim_red_method.lower() == 'pca':
            img_reducer = PCA(n_components=2, random_state=SEED)
            text_reducer = PCA(n_components=2, random_state=SEED+1)
        elif dim_red_method.lower() == 'umap':
            img_reducer = UMAP(n_components=2, random_state=SEED)
            text_reducer = UMAP(n_components=2, random_state=SEED+1)
        elif dim_red_method.lower() == 'tsne':
            img_reducer = TSNE(n_components=2, random_state=SEED)
            text_reducer = TSNE(n_components=2, random_state=SEED+1)

        img_embeddings_reduced = img_reducer.fit_transform(img_embeddings_numpy)
        text_embeddings_reduced = text_reducer.fit_transform(text_embeddings_numpy)
    else:
        combined_reduced = reducer.fit_transform(combined_embeddings)
        img_embeddings_reduced = combined_reduced[:img_embeddings_numpy.shape[0]]
        text_embeddings_reduced = combined_reduced[img_embeddings_numpy.shape[0]:]

    print(f"Final image embeddings: {img_embeddings_reduced.shape}")
    print(f"Final text embeddings: {text_embeddings_reduced.shape}")

    return (
        img_embeddings_reduced,
        text_embeddings_reduced,
        numerosity_array_final,
        obj_type_array_final
    )

def process_visual_embeddings(
   img_embeddings,
   numerosity_array,
   obj_type_array,
   max_count,
   numerosities=None,
   object_types=None,
   dim_red_method='svd',
   pre_reduction=False,
   SEED=42,
   pre_reduction_dim=400,
   normalize=True
):

   emb_dim = img_embeddings.shape[-1]
   if emb_dim == 4096:
       print('LLaVa embeddings detected!')
   elif emb_dim == 1024:
       print('LLaVa Unprojected embeddings detected!')
   elif emb_dim == 512:
       img_embeddings = img_embeddings[:, 0, :]  # Use CLS token
       print('CLIP embeddings detected!')
   else:
       raise ValueError("Unsupported embedding dimension")


   mask = np.ones(len(numerosity_array), dtype=bool)
   if numerosities is not None:
       mask &= np.isin(numerosity_array, numerosities)
   if object_types is not None:
       mask &= np.isin(obj_type_array, object_types)

   img_embeddings_numpy = img_embeddings[mask].cpu().numpy()
   numerosity_array_filtered = np.array(numerosity_array)[mask]
   obj_type_array_filtered = np.array(obj_type_array)[mask]


   np.random.seed(SEED)
   indices = np.random.permutation(len(img_embeddings_numpy))
   img_embeddings_numpy = img_embeddings_numpy[indices]
   numerosity_array_filtered = numerosity_array_filtered[indices]
   obj_type_array_filtered = obj_type_array_filtered[indices]


   max_count = min(max_count, len(img_embeddings_numpy))
   img_embeddings_numpy = img_embeddings_numpy[:max_count]
   numerosity_array_img = numerosity_array_filtered[:max_count]
   obj_type_array_img = obj_type_array_filtered[:max_count]

   img_embeddings_numpy = img_embeddings_numpy.reshape(-1, emb_dim)
   print(f"Image embeddings: {img_embeddings_numpy.shape}")


   if normalize:
       img_embeddings_numpy = img_embeddings_numpy / np.linalg.norm(img_embeddings_numpy, axis=-1, keepdims=True)
       print("Applied L2 normalization to embeddings")


   if pre_reduction:
       svd = TruncatedSVD(n_components=pre_reduction_dim, random_state=SEED)
       print(f"Applying SVD pre-reduction to {pre_reduction_dim} dimensions...")
       img_embeddings_numpy = svd.fit_transform(img_embeddings_numpy)


   img_embeddings_reduced = reduce_dimension(img_embeddings_numpy, 2, dim_red_method, SEED)

   return img_embeddings_reduced, numerosity_array_img, obj_type_array_img

####------########------########------########------########------########------########------########------####
#  The plotting functions for multimodality gap analysis
####------########------########------########------########------########------########------########------####

# The plotter functions. They require usage of the processing functions defined above to correctly plot.

def plot_visual_embedding_scatter(
    visual_embeddings,
    numerosity_array,
    obj_type_array,
    title="Visual Embedding Scatter Plot",
    border=None,
    averages=False,
    plot_convex=False
):

    print(f"[INFO] Embeddings shape: {visual_embeddings.shape}")
    print(f"[INFO] Numerosity array length: {len(numerosity_array)}")
    print(f"[INFO] Object type array length: {len(obj_type_array)}")

    assert visual_embeddings.shape[0] == len(numerosity_array) == len(obj_type_array), \
        "[ERROR] Length mismatch between embeddings, numerosity_array, and obj_type_array."
    assert visual_embeddings.shape[1] == 2, \
        "[ERROR] visual_embeddings must be 2D (N x 2) for scatter plot and convex hull."

    cmap = cm.viridis_r
    num_cats = np.unique(numerosity_array)

    fig, ax = plt.subplots(figsize=(10, 8))

    for i, category in enumerate(num_cats):
        idxs = np.where(numerosity_array == category)
        color = cmap(i / len(num_cats))

        if averages:
            avg_x = np.mean(visual_embeddings[idxs, 0])
            avg_y = np.mean(visual_embeddings[idxs, 1])
            ax.scatter(
                avg_x,
                avg_y,
                label=f'Number {category} (avg)',
                s=100,
                color=color,
                marker='x',
                linewidths=3,
                alpha=0.9
            )
        else:
            ax.scatter(
                visual_embeddings[idxs, 0],
                visual_embeddings[idxs, 1],
                label=f'Number {category}',
                s=100,
                color=color,
                alpha=0.7
            )

    if plot_convex:
        obj_types = np.unique(obj_type_array)
        hull_colors = cm.Set3(np.linspace(0, 1, len(obj_types)))

        for obj_type, hull_color in zip(obj_types, hull_colors):
            obj_idxs = np.where(obj_type_array == obj_type)[0]
            points = visual_embeddings[obj_idxs]

            print(f"[DEBUG] Object type: {obj_type}, Points: {len(points)}")

            if len(points) < 3:
                print(f"[WARN] Skipping {obj_type}: Not enough points for a convex hull.")
                continue

            try:
                hull = ConvexHull(points)
                for simplex in hull.simplices:
                    ax.plot(points[simplex, 0], points[simplex, 1],
                            color=hull_color, alpha=0.6, linewidth=2)

                hull_points = points[hull.vertices]
                hull_patch = patches.Polygon(hull_points, alpha=0.1, facecolor=hull_color)
                ax.add_patch(hull_patch)

                centroid_x = np.mean(hull_points[:, 0])
                centroid_y = np.mean(hull_points[:, 1])
                ax.text(centroid_x, centroid_y, str(obj_type),
                        fontsize=10, fontweight='bold', ha='center', va='center')

            except Exception as e:
                print(f"[ERROR] Convex hull failed for object type {obj_type}: {e}")

    ax.set_xlabel('Embedding Dim 1')
    ax.set_ylabel('Embedding Dim 2')
    ax.set_title(title)
    ax.grid(True)
    ax.legend()

    if border is not None:
        if isinstance(border, (list, tuple)) and len(border) == 4:
            ax.set_xlim(border[0], border[1])
            ax.set_ylim(border[2], border[3])
        elif border is False:
            for spine in ax.spines.values():
                spine.set_visible(False)

    plt.tight_layout()
    plt.show()


def plot_multimodal_embedding_scatter(
    img_embeddings,
    text_embeddings,
    numerosity_array,
    obj_type_array,
    title="Multimodal Embedding Scatter Plot",
    border=None,
    averages=False,
    plot_convex=False,
    show_text_labels=False,
    show_images=True,
    show_texts=True
):



    cmap = cm.viridis_r
    num_cats = np.unique(numerosity_array)
    fig, ax = plt.subplots(figsize=(12, 8))

    # Plot image and text embeddings
    for i, category in enumerate(num_cats):
        idxs = np.where(numerosity_array == category)
        color = cmap(i / len(num_cats))

        avg_text_x = np.mean(text_embeddings[idxs, 0])
        avg_text_y = np.mean(text_embeddings[idxs, 1])
        avg_img_x = np.mean(img_embeddings[idxs, 0])
        avg_img_y = np.mean(img_embeddings[idxs, 1])

        if averages:

            if show_images:
                ax.scatter(avg_img_x, avg_img_y, label=f'Img {int(category)} (avg)',
                          s=100, color=color, marker='x', linewidths=3, alpha=0.9)


            if show_texts:
                ax.scatter(avg_text_x, avg_text_y, label=f'Text {int(category)} (avg)',
                          s=100, color=color, marker='+', linewidths=3, alpha=0.9)
        else:

            if show_images:
                ax.scatter(img_embeddings[idxs, 0], img_embeddings[idxs, 1],
                          label=f'Numerosity {int(category)}', s=100, color=color, alpha=0.7, marker='o')


            if show_texts:
                ax.scatter(text_embeddings[idxs, 0], text_embeddings[idxs, 1],
                          s=100, color=color, alpha=0.7, marker='^')


            if show_text_labels and show_texts:
                for idx in idxs[0]:
                    ax.text(text_embeddings[idx, 0], text_embeddings[idx, 1],
                           str(int(numerosity_array[idx])),
                           fontsize=8, ha='center', va='center',
                           bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.7))


    if show_texts:
        for i, category in enumerate(num_cats):
            idxs = np.where(numerosity_array == category)
            avg_text_x = np.mean(text_embeddings[idxs, 0])
            avg_text_y = np.mean(text_embeddings[idxs, 1])
            ax.text(avg_text_x, avg_text_y + 0.3, str(int(category)),
                   fontsize=12, fontweight='bold', ha='center', va='center')


    if show_images and show_texts:
        ax.text(avg_text_x+2, avg_text_y+4.5, 'Text Embeddings',
                fontsize=14, fontweight='bold', ha='center', va='center')
        ax.text(avg_img_x, avg_img_y, 'Image Embeddings',
                fontsize=14, fontweight='bold', ha='center', va='center')


    if plot_convex and show_images:
        obj_types = np.unique(obj_type_array)
        hull_colors = cm.Set3(np.linspace(0, 1, len(obj_types)))

        for obj_type, hull_color in zip(obj_types, hull_colors):
            obj_idxs = np.where(obj_type_array == obj_type)[0]
            if len(obj_idxs) >= 3:
                points = img_embeddings[obj_idxs]
                try:
                    hull = ConvexHull(points)

                    for simplex in hull.simplices:
                        ax.plot(points[simplex, 0], points[simplex, 1],
                                color=hull_color, alpha=0.6, linewidth=2)

                    hull_points = points[hull.vertices]
                    hull_patch = patches.Polygon(hull_points, alpha=0.1,
                                                 facecolor=hull_color)
                    ax.add_patch(hull_patch)

                    centroid_x = np.mean(hull_points[:, 0])
                    centroid_y = np.mean(hull_points[:, 1])
                    ax.text(centroid_x, centroid_y, str(obj_type),
                            fontsize=10, fontweight='bold', ha='center', va='center')
                except:
                    pass

    # Final touches
    ax.set_xlabel('Embedding Dim 1')
    ax.set_ylabel('Embedding Dim 2')
    ax.set_title(title)
    ax.grid(True)
    ax.legend()


    if border is not None:
        if isinstance(border, (list, tuple)) and len(border) == 4:
            ax.set_xlim(border[0], border[1])
            ax.set_ylim(border[2], border[3])
        elif border == False:
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['bottom'].set_visible(False)
            ax.spines['left'].set_visible(False)

    plt.tight_layout()
    plt.show()


def plot_text_embedding_scatter(
    text_embeddings,
    numerosity_array,
    title="Text Embedding Scatter Plot",
    border=None,
    averages=False
):
    cmap = cm.viridis_r
    num_cats = np.unique(numerosity_array)

    fig, ax = plt.subplots(figsize=(10, 8))

    for i, category in enumerate(num_cats):
        idxs = np.where(numerosity_array == category)
        color = cmap(i / len(num_cats))

        if averages:
            avg_x = np.mean(text_embeddings[idxs, 0])
            avg_y = np.mean(text_embeddings[idxs, 1])

            ax.scatter(
                avg_x,
                avg_y,
                label=f'Number {category} (avg)',
                s=100,
                color=color,
                marker='x',
                linewidths=3,
                alpha=0.9
            )

            x_range = ax.get_xlim()[1] - ax.get_xlim()[0]
            y_range = ax.get_ylim()[1] - ax.get_ylim()[0]
            ax.text(avg_x + 0.02 * x_range, avg_y + 0.02 * y_range, str(int(category)), fontsize=14, weight='bold')
        else:
            x_coords = text_embeddings[idxs, 0]
            y_coords = text_embeddings[idxs, 1]

            ax.scatter(
                x_coords,
                y_coords,
                label=f'Number {category}',
                s=50,
                color=color,
                alpha=0.7
            )

            center_x = np.mean(x_coords)
            center_y = np.mean(y_coords)
            x_range = np.ptp(x_coords) if len(x_coords) > 1 else 1
            y_range = np.ptp(y_coords) if len(y_coords) > 1 else 1
            ax.text(center_x + 0.1 * x_range, center_y + 0.1 * y_range, str(int(category)), fontsize=14, weight='bold')

    ax.set_xlabel('Embedding Dim 1')
    ax.set_ylabel('Embedding Dim 2')
    ax.set_title(title)
    ax.grid(True)
    ax.legend()

    if border is not None:
        if isinstance(border, (list, tuple)) and len(border) == 4:
            ax.set_xlim(border[0], border[1])
            ax.set_ylim(border[2], border[3])
        elif border == False:
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['bottom'].set_visible(False)
            ax.spines['left'].set_visible(False)

    plt.tight_layout()
    plt.show()



####------########------########------########------########------########------########------########------####
# Helper functions for modality gap analysis
####------########------########------########------########------########------########------########------####

def generate_prompt_embeddings(
    number_array,
    image_PIL,
    pipe_object_llava,
    duplications_per_prompt=1,
    max_length=13,
    use_after_attention=True
):
    unique_prompts = [f'A photo of {i} objects.' for i in number_array]

    all_embeddings = []
    all_labels = []

    for i, prompt in enumerate(unique_prompts):
        text_emb_after, text_emb_before = get_text_embeddings(prompt, image_PIL, pipe_object_llava, max_length)
        print(f"Text embeddings after: {text_emb_after.shape}")
        print(f"Text embeddings before: {text_emb_before.shape}")

        selected_emb = text_emb_after if use_after_attention else text_emb_before

        for _ in range(duplications_per_prompt):
            all_embeddings.append(selected_emb)
            all_labels.append(number_array[i])

    concatenated_embeddings = np.stack(all_embeddings, axis=0)
    prompt_labels = np.array(all_labels)

    return concatenated_embeddings, prompt_labels



def generate_custom_prompt_embeddings(
    prompt,
    image_PIL,
    pipe_object_llava,
    duplications=1,
    max_length=13,
    use_after_attention=True
):

    text_emb_after, text_emb_before = get_text_embeddings(prompt, image_PIL, pipe_object_llava, max_length)
    selected_emb = text_emb_after if use_after_attention else text_emb_before

    all_embeddings = [selected_emb for _ in range(duplications)]
    concatenated_embeddings = np.stack(all_embeddings, axis=0)

    return concatenated_embeddings
    
    
    
def plot_patch_angle_diff_heatmap(untuned_embeddings, tuned_embeddings, cmap="viridis"):
    untuned_norm = torch.nn.functional.normalize(untuned_embeddings, dim=-1)
    tuned_norm = torch.nn.functional.normalize(tuned_embeddings, dim=-1)

    cos_sim = torch.sum(untuned_norm * tuned_norm, dim=-1)
    cos_sim = torch.clamp(cos_sim, -1.0, 1.0)

    angles = torch.acos(cos_sim)
    avg_angles = angles.mean(dim=0)

    heatmap = torch.abs(avg_angles[:, None] - avg_angles[None, :])

    plt.figure(figsize=(10, 8))
    plt.imshow(heatmap.numpy(), cmap=cmap)
    plt.colorbar(label="Average angle difference (rad)")
    plt.title("Average Angle Differences Between Patch Embeddings")
    plt.xlabel("Patch Index")
    plt.ylabel("Patch Index")
    plt.show()

    return heatmap

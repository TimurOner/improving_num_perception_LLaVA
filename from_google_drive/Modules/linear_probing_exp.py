# -*- coding: utf-8 -*-
"""linear_probing_exp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m0-JiBBZEU-pAosXeS2TRsrMFvr00c4g
"""

# Built-in libraries
import os,sys
import json
import random
import re
from typing import Dict, List, Optional, Tuple, Union
from tqdm import tqdm


# Scientific computing
import numpy as np
import pandas as pd


# PyTorch
import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader,random_split, Subset
from torch.optim import Adam
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau ,CosineAnnealingLR


# Visualization
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from IPython.display import display, clear_output
from matplotlib.lines import Line2D
import seaborn as sns
from matplotlib.pyplot import Normalize
import matplotlib.patches as patches

# Machine Learning / Dimensionality Reduction
from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



# Global Constants for Better Reproduciblity

SEED = 42


# Making PyTorch more reproducable

torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.use_deterministic_algorithms(True)

# numpy

np.random.seed(SEED)

from google.colab import drive
drive.mount('/content/drive')

modules_dir = '/content/drive/MyDrive/Modality Gap and Numerosity Thesis/Modules'
sys.path.append(modules_dir)

# Now import the module (make sure the module name matches the .py file)
from modality_vis_functions import *
from data_utils import *
from probing_utils import *

help(reduce_dimension)

# I could not decide where to put this function yet lol
def get_all_predictions(model, dataloader, device='cpu'):
    model.eval()
    model.to(device)

    all_logits = []
    all_probs = []
    all_preds = []

    with torch.no_grad():
        for x, _ in dataloader:
            x = x.to(device)
            logits, probs = model(x)
            preds = torch.argmax(logits, dim=1) + 1

            all_logits.extend(logits.cpu().tolist())
            all_probs.extend(probs.cpu().tolist())
            all_preds.extend(preds.cpu().tolist())

    return all_logits, all_probs, all_preds






# This too lol
def create_clean_training_setup(model, device, hparams):
    model = fix_model_dtype(model, device)
    model.train()

    optimizer = optim.AdamW(
        model.parameters(),
        lr=hparams.get("learning_rate", 3e-4),
        weight_decay=hparams.get("weight_decay", 1e-4),
        eps=hparams.get("eps", 1e-8),
        betas=hparams.get("betas", (0.9, 0.999))
    )

    scheduler = CosineAnnealingLR(
        optimizer,
        T_max=hparams.get("T_max", 50),
        eta_min=hparams.get("eta_min", 1e-6)
    )

    return model, optimizer, scheduler

path_tuned_proj_emb = "/content/drive/MyDrive/Modality Gap and Numerosity Thesis/Experiments/Exp3/embeddings/proj_cls_embeddings_tuned.pt"
path_untuned_proj_emb = "/content/drive/MyDrive/Modality Gap and Numerosity Thesis/Experiments/Exp3/embeddings/proj_cls_embeddings_untuned.pt"
gt_nums_path = "/content/drive/MyDrive/Modality Gap and Numerosity Thesis/Experiments/Exp3/val_set_preds/val_gts.npy"

# Load the tensors
tuned_proj_emb = torch.load(path_tuned_proj_emb)
untuned_proj_emb = torch.load(path_untuned_proj_emb)
gt_nums = np.load(gt_nums_path)
# Optional: check their shapes
print("Tuned projection embedding shape:", tuned_proj_emb.shape if hasattr(tuned_proj_emb, 'shape') else type(tuned_proj_emb))
print("Untuned projection embedding shape:", untuned_proj_emb.shape if hasattr(untuned_proj_emb, 'shape') else type(untuned_proj_emb))
print("Ground truth labels shape:", gt_nums.shape if hasattr(gt_nums, 'shape') else type(gt_nums))

import numpy as np
import matplotlib.pyplot as plt

# Compute absolute differences
abs_diff = np.abs(tuned_proj_emb - untuned_proj_emb)

# Average absolute difference per sample (raw)
avg_abs_diff_per_sample = abs_diff.mean(axis=1)

# Compute mean magnitude per sample
mean_magnitude_per_sample = np.abs(tuned_proj_emb).mean(axis=1)

# Convert to percentage relative to mean magnitude of embeddings
percent_diff_per_sample = (avg_abs_diff_per_sample / mean_magnitude_per_sample) * 100

# Statistics for raw absolute differences
mean_abs = avg_abs_diff_per_sample.mean()
min_abs = avg_abs_diff_per_sample.min()
max_abs = avg_abs_diff_per_sample.max()
std_abs = avg_abs_diff_per_sample.std()

# Statistics for percentual differences
mean_percent = percent_diff_per_sample.mean()
min_percent = percent_diff_per_sample.min()
max_percent = percent_diff_per_sample.max()
std_percent = percent_diff_per_sample.std()

# Statistics for mean magnitude of embeddings
mean_mag = mean_magnitude_per_sample.mean()
min_mag = mean_magnitude_per_sample.min()
max_mag = mean_magnitude_per_sample.max()
std_mag = mean_magnitude_per_sample.std()

# Print results
print("=== Raw Average Absolute Differences ===")
print(f"Mean: {mean_abs:.6f}")
print(f"Min: {min_abs:.6f}")
print(f"Max: {max_abs:.6f}")
print(f"Std: {std_abs:.6f}")

print("\n=== Percentual Differences ===")
print(f"Mean: {mean_percent:.4f}%")
print(f"Min: {min_percent:.4f}%")
print(f"Max: {max_percent:.4f}%")
print(f"Std: {std_percent:.4f}%")

print("\n=== Mean Magnitude of Tuned Embeddings ===")
print(f"Mean: {mean_mag:.6f}")
print(f"Min: {min_mag:.6f}")
print(f"Max: {max_mag:.6f}")
print(f"Std: {std_mag:.6f}")

# Plot histogram of percentual differences
plt.figure(figsize=(8,5))
plt.hist(percent_diff_per_sample, bins=30, color='skyblue', edgecolor='black')
plt.title("Histogram of Percentual Differences per Sample")
plt.xlabel("Percent Difference (%)")
plt.ylabel("Number of Samples")
plt.grid(axis='y', alpha=0.75)
plt.show()

tuned_proj_emb.shape

"""A short check of relative values differences of CLS tokens from tuned and untuned projections."""

tuned_proj_emb = torch.load(path_tuned_proj_emb)
untuned_proj_emb = torch.load(path_untuned_proj_emb)
gt_nums = np.load(gt_nums_path)

# Optinal dimension reduction step

tuned_proj_emb = reduce_dimension(tuned_proj_emb , 128 , dim_reduction_method = 'svd', SEED = SEED)
untuned_proj_emb = reduce_dimension(untuned_proj_emb, 128, dim_reduction_method = 'svd', SEED = SEED)

# Creating the datasets
tuned_splits = create_stratified_datasets(tuned_proj_emb, gt_nums[0])
tuned_train_dataset = tuned_splits['train_dataset']
tuned_val_dataset = tuned_splits['val_dataset']
tuned_test_dataset = tuned_splits['test_dataset']
train_labesl = tuned_splits['train_labels']
val_labels = tuned_splits['val_labels']
test_labels = tuned_splits['test_labels']

# For untuned embeddings
untuned_splits = create_stratified_datasets(untuned_proj_emb, gt_nums[0])
untuned_train_dataset = untuned_splits['train_dataset']
untuned_val_dataset = untuned_splits['val_dataset']
untuned_test_dataset = untuned_splits['test_dataset']

tuned_train_loader = DataLoader(tuned_train_dataset, batch_size=32, shuffle=True)
tuned_val_loader = DataLoader(tuned_val_dataset, batch_size=32, shuffle=False)
tuned_test_loader = DataLoader(tuned_test_dataset, batch_size=32, shuffle=False)

untuned_train_loader = DataLoader(untuned_train_dataset, batch_size=32, shuffle=True)
untuned_val_loader = DataLoader(untuned_val_dataset, batch_size=32, shuffle=False)
untuned_test_loader = DataLoader(untuned_test_dataset, batch_size=32, shuffle=False)

# Optional Dimension Reducing Step

hparams = {
    "learning_rate": 1e-3,
    "weight_decay": 1e-4,
    "betas": (0.9, 0.999),
    "eps": 1e-8,
    "T_max": 25,
    "eta_min": 1e-6,
    "batch_size": 32,
    "num_epochs": 200,
    "patience": 8
}



model_tuned =  NumerosityLinearProbeCLS(input_dim=128)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_tuned, optimizer, scheduler = create_clean_training_setup(model_tuned, device,hparams)


trained_model_tuned, train_losses_tuned, val_losses_tuned = simple_train_model(
   model=model_tuned,
   train_loader=tuned_train_loader,
   val_loader=tuned_val_loader,
   optimizer=optimizer,
   scheduler=scheduler,
   device=device,
   max_epochs=hparams['num_epochs'],
   patience=hparams['patience']
)

model_untuned =  NumerosityLinearProbeCLS(input_dim=128)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_untuned, optimizer, scheduler = create_clean_training_setup(model_untuned, device,hparams)
trained_model_untuned, train_losses_untuned, val_losses_untuned = simple_train_model( model=model_untuned,
   train_loader=untuned_train_loader,
   val_loader=untuned_val_loader,
   optimizer=optimizer,
   scheduler=scheduler,
   device=device,
   max_epochs=hparams['num_epochs'],
   patience=hparams['patience'])

trained_tuned_logits,trained_tuned_probs,trained_tuned_preds  =get_all_predictions(model_tuned , tuned_test_loader, device='cpu')
trained_untuned_logits,trained_untuned_probs,trained_untuned_preds = get_all_predictions(model_untuned , untuned_test_loader, device='cpu')

plot_preds_vs_gts(trained_tuned_preds ,test_labels )

plot_preds_vs_gts(trained_untuned_preds ,test_labels )

"""Now we will try to quantify how different are patch embeddings!"""

proj_patch_embeddings_tuned_path = '/content/drive/MyDrive/Modality Gap and Numerosity Thesis/finetuning/Exp3/patch_embeddings/proj_patch_embeddings_tuned.pt'
proj_patch_embeddings_untuned_path = '/content/drive/MyDrive/Modality Gap and Numerosity Thesis/finetuning/Exp3/patch_embeddings/proj_patch_embeddings_untuned.pt'

tuned_proj_patch_embeddings = torch.load(proj_patch_embeddings_tuned_path)
untuned_proj_patch_embeddings = torch.load(proj_patch_embeddings_untuned_path)

gt_nums = np.load(gt_nums_path)

def plot_patchwise_angle_heatmap(emb1, emb2, grid_size=24, cmap="viridis"):
    emb1 = torch.tensor(emb1, dtype=torch.float32)
    emb2 = torch.tensor(emb2, dtype=torch.float32)

    emb1 = torch.nn.functional.normalize(emb1, dim=-1)
    emb2 = torch.nn.functional.normalize(emb2, dim=-1)

    cos_sim = (emb1 * emb2).sum(dim=-1).clamp(-1.0, 1.0)
    angles = torch.acos(cos_sim)  # radians
    angles_2d = angles.view(grid_size, grid_size)

    plt.figure(figsize=(8, 6))
    plt.imshow(angles_2d.numpy(), cmap=cmap)
    plt.colorbar(label="Angle difference (rad)")
    plt.title(f"{grid_size}x{grid_size} Patchwise Angle Differences")
    plt.xlabel("Grid X")
    plt.ylabel("Grid Y")
    plt.show()

    return angles_2d.numpy()

# Optinal dimension reduction step

tuned_proj_emb = reduce_dimension(tuned_proj_patch_embeddings, 128 , dim_reduction_method = 'pca', SEED = SEED)
untuned_proj_emb = reduce_dimension(untuned_proj_patch_embeddings, 128, dim_reduction_method = 'pca', SEED = SEED)

# For tuned embeddings (patch + flatten)
tuned_patch_splits = create_stratified_datasets(tuned_proj_patch_embeddings, gt_nums[0], patch=True, flatten=True)
tuned_patch_train_dataset = tuned_patch_splits['train_dataset']
tuned_patch_val_dataset = tuned_patch_splits['val_dataset']
tuned_patch_test_dataset = tuned_patch_splits['test_dataset']
train_patch_labels = tuned_patch_splits['train_labels']
val_patch_labels = tuned_patch_splits['val_labels']
test_patch_labels = tuned_patch_splits['test_labels']

# # For untuned embeddings (patch + flatten)
untuned_patch_splits = create_stratified_datasets(untuned_proj_patch_embeddings,gt_nums[0], patch=True, flatten=True)
untuned_patch_train_dataset = untuned_patch_splits['train_dataset']
untuned_patch_val_dataset = untuned_patch_splits['val_dataset']
untuned_patch_test_dataset = untuned_patch_splits['test_dataset']

# DataLoaders
tuned_patch_train_loader = DataLoader(tuned_patch_train_dataset, batch_size=32, shuffle=True)
tuned_patch_val_loader = DataLoader(tuned_patch_val_dataset, batch_size=32, shuffle=False)
tuned_patch_test_loader = DataLoader(tuned_patch_test_dataset, batch_size=32, shuffle=False)

untuned_patch_train_loader = DataLoader(untuned_patch_train_dataset, batch_size=32, shuffle=True)
untuned_patch_val_loader = DataLoader(untuned_patch_val_dataset, batch_size=32, shuffle=False)
untuned_patch_test_loader = DataLoader(untuned_patch_test_dataset, batch_size=32, shuffle=False)

plot_patchwise_angle_heatmap(tuned_proj_patch_embeddings[60], untuned_proj_patch_embeddings[60] , grid_size=24, cmap="viridis")

# Optional Dimension Reducing Step for the patch embeddings

"""Now training Linear Probes on all of the patch embeddings to see if tuning improved the features of projected patch embeddings."""

hparams = {
    "learning_rate": 1e-7,
    "weight_decay": 1e-4,
    "betas": (0.9, 0.999),
    "eps": 1e-8,
    "T_max": 25,
    "eta_min": 1e-6,
    "batch_size": 32,
    "num_epochs": 25,
    "patience": 8
}

model_untuned_patch =  NumerosityLinearProbeFlattenPatch()

model_untuned_patch, optimizer, scheduler = create_clean_training_setup(model_untuned_patch, device,hparams)


trained_model_tuned, train_losses_tuned, val_losses_tuned = simple_train_model(
   model=model_untuned_patch ,
   train_loader=untuned_patch_train_loader,
   val_loader=untuned_patch_val_loader,
   optimizer=optimizer,
   scheduler=scheduler,
   device=device,
   max_epochs=hparams["num_epochs"],
   patience=hparams["patience"]
)



hparams = {
    "learning_rate": 1e-7,
    "weight_decay": 1e-4,
    "betas": (0.9, 0.999),
    "eps": 1e-8,
    "T_max": 25,
    "eta_min": 1e-6,
    "batch_size": 32,
    "num_epochs": 25,
    "patience": 8
}

model_tuned_patch =  NumerosityLinearProbeFlattenPatch()

model_tuned_patch, optimizer, scheduler = create_clean_training_setup(model_tuned_patch, device,hparams)


trained_model_tuned, train_losses_tuned, val_losses_tuned = simple_train_model(
   model=model_tuned_patch ,
   train_loader=tuned_patch_train_loader,
   val_loader=tuned_patch_val_loader,
   optimizer=optimizer,
   scheduler=scheduler,
   device=device,
   max_epochs=hparams["num_epochs"],
   patience=hparams["patience"]
)

trained_tuned_logits,trained_tuned_probs,trained_tuned_preds  =get_all_predictions(model_tuned_patch , tuned_patch_test_loader, device='cpu')
trained_untuned_logits,trained_untuned_probs,trained_untuned_preds  =get_all_predictions(model_untuned_patch , untuned_patch_test_loader, device='cpu')

plot_preds_vs_gts(trained_tuned_preds ,test_patch_labels )

plot_preds_vs_gts(trained_untuned_preds ,test_patch_labels )
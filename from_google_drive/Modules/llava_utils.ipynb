{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOW1ZsM9obd9toSV4rPI3OB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import re\n","\n","from transformers import AutoTokenizer, AutoProcessor, AutoModelForCausalLM"],"metadata":{"id":"pmc-7_-p_Llt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VD4HZ9zz-6w-"},"outputs":[],"source":["def check_lora_weight_diff(baseline_model, finetuned_model, atol=1e-6):\n","    modules_to_check = [\n","        \"model.multi_modal_projector.linear_1\",\n","        \"model.multi_modal_projector.linear_2\",\n","    ]\n","\n","    baseline_params = dict(baseline_model.named_parameters())\n","    finetuned_params = dict(finetuned_model.named_parameters())\n","\n","    for module_name in modules_to_check:\n","        if f\"{module_name}.weight\" not in baseline_params:\n","            print(f\"‚ö†Ô∏è Skipping {module_name}, not found in baseline model\")\n","            continue\n","        if f\"{module_name}.weight\" not in finetuned_params:\n","            print(f\"‚ö†Ô∏è Skipping {module_name}, not found in finetuned model\")\n","            continue\n","\n","        baseline_weight = baseline_params[f\"{module_name}.weight\"].detach().cpu().float()\n","        finetuned_weight = finetuned_params[f\"{module_name}.weight\"].detach().cpu().float()\n","\n","        diff = (finetuned_weight - baseline_weight).abs()\n","        max_diff = diff.max().item()\n","        mean_diff = diff.mean().item()\n","\n","        print(f\"üîç Checking {module_name}:\")\n","        print(f\"   Max abs diff:  {max_diff:.6e}\")\n","        print(f\"   Mean abs diff: {mean_diff:.6e}\")\n","        if torch.allclose(baseline_weight, finetuned_weight, atol=atol):\n","            print(\"   ‚úÖ No significant changes (within tolerance)\\n\")\n","        else:\n","            print(\"   ‚ö†Ô∏è Weights changed (LoRA seems applied)\\n\")\n","\n","\n","\n","\n","def manual_inference(model, processor, image, prompt):\n","\n","\n","    inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n","\n","    with torch.no_grad():\n","        output = model.generate(\n","            **inputs,\n","            max_new_tokens=50,\n","            do_sample=False,\n","\n","        )\n","\n","\n","    generated_text = processor.decode(output[0], skip_special_tokens=True)\n","    return generated_text"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCRiRftKR89zjm/PyZmKCM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HWiOToDMB_up"},"outputs":[],"source":[]},{"cell_type":"code","source":["def train_vit_classifier(\n","    model,\n","    train_dataset,\n","    val_dataset,\n","    device,\n","    num_epochs=10,\n","    batch_size=32,\n","    lr=1e-4\n","):\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(1, num_epochs + 1):\n","        # --- Training ---\n","        model.train()\n","        train_loss = 0.0\n","        correct_train = 0\n","        total_train = 0\n","\n","        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","            logits = model(images)\n","            loss = criterion(logits, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item() * images.size(0)\n","            preds = logits.argmax(dim=1)\n","            correct_train += (preds == labels).sum().item()\n","            total_train += labels.size(0)\n","\n","        train_loss /= total_train\n","        train_acc = correct_train / total_train\n","\n","        # --- Validation ---\n","        model.eval()\n","        val_loss = 0.0\n","        correct_val = 0\n","        total_val = 0\n","\n","        with torch.no_grad():\n","            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n","                images = images.to(device)\n","                labels = labels.to(device)\n","\n","                logits = model(images)\n","                loss = criterion(logits, labels)\n","\n","                val_loss += loss.item() * images.size(0)\n","                preds = logits.argmax(dim=1)\n","                correct_val += (preds == labels).sum().item()\n","                total_val += labels.size(0)\n","\n","        val_loss /= total_val\n","        val_acc = correct_val / total_val\n","\n","        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n","              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n","\n","\n","\n","class ViTClassifier(nn.Module):\n","    def __init__(self, vit_model, num_classes, use_patch=False, svd=False, svd_dim=256):\n","        super(ViTClassifier, self).__init__()\n","        self.vit = vit_model\n","        self.use_patch = use_patch\n","        self.svd = svd\n","        self.svd_dim = svd_dim\n","        base_feature_dim = vit_model.embeddings.patch_embedding.out_channels\n","        if use_patch:\n","            num_patches = vit_model.embeddings.position_embedding.num_embeddings - 1\n","            self.feature_dim = num_patches * base_feature_dim\n","        else:\n","            self.feature_dim = base_feature_dim\n","        classifier_input_dim = svd_dim if svd else self.feature_dim\n","        if svd:\n","            self.svd_projection = None\n","        self.classifier = nn.Linear(classifier_input_dim, num_classes)\n","\n","    def fit_svd(self, train_features):\n","        if not self.svd:\n","            raise ValueError(\"SVD is not enabled for this model\")\n","        U, S, Vt = torch.svd(train_features.cpu())\n","        self.svd_projection = Vt[:self.svd_dim, :].T\n","        self.svd_projection = nn.Parameter(\n","            self.svd_projection.to(train_features.device),\n","            requires_grad=False\n","        )\n","        print(f\"SVD fitted: {train_features.shape[1]} -> {self.svd_dim} dimensions\")\n","\n","    def forward(self, x):\n","        outputs = self.vit(x)\n","        hidden_states = outputs.last_hidden_state\n","        if self.use_patch:\n","            features = hidden_states[:, 1:, :]\n","            B, N, D = features.shape\n","            features = features.reshape(B, N * D)\n","        else:\n","            features = hidden_states[:, 0, :]\n","        if self.svd:\n","            if self.svd_projection is None:\n","                raise RuntimeError(\"SVD is enabled but not fitted. Call fit_svd() first!\")\n","            features = features @ self.svd_projection\n","        logits = self.classifier(features)\n","        return logits\n","\n","\n","class ImageDatasetVit(Dataset):\n","    def __init__(self, image_base_path, nums=None, categories=None):\n","        self.image_paths = []\n","        self.labels = []\n","\n","        all_img_files = os.listdir(image_base_path)\n","\n","        for img_file in all_img_files:\n","            true_num = self._get_true_num(img_file)\n","            cat_name = self._get_category(img_file)\n","\n","            if (nums is None or true_num in nums) and (categories is None or cat_name in categories):\n","                self.image_paths.append(os.path.join(image_base_path, img_file))\n","                self.labels.append(true_num)\n","\n","    def _get_true_num(self, img_name):\n","        parts = img_name.split('_')\n","        for part in parts:\n","            if part.isdigit() and int(part) < 100:\n","                return int(part)\n","        return None\n","\n","    def _get_category(self, img_name):\n","        return img_name.split('_')[0]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image = Image.open(image_path).convert(\"RGB\")\n","        label = self.labels[idx]\n","        return image, label\n","\n","def partially_unfreeze_vit(model, unfreeze_last_n_layers=4):\n","\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","\n","    for layer in model.encoder.layers[-unfreeze_last_n_layers:]:\n","        for param in layer.parameters():\n","            if param.is_floating_point():  # <-- only set requires_grad if float\n","                param.requires_grad = True\n","\n","    # Count parameters\n","    total_params = sum(p.numel() for p in model.parameters())\n","    frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n","    unfrozen_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","    print(f\"Total parameters: {total_params}\")\n","    print(f\"Frozen parameters: {frozen_params}\")\n","    print(f\"Unfrozen parameters: {unfrozen_params}\")\n","\n","# Example usage\n","partially_unfreeze_vit(baseline_vit, unfreeze_last_n_layers=4)\n","\n","\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n","\n","def setup_peft_vit_mlp(model, r=8, lora_alpha=16, lora_dropout=0.1,\n","                       target_modules=None, use_gradient_checkpointing=True):\n","\n","    # Step 1: Prepare the quantized model for training\n","    model = prepare_model_for_kbit_training(\n","        model,\n","        use_gradient_checkpointing=use_gradient_checkpointing\n","    )\n","\n","    # Step 2: Define target modules for CLIP ViT MLP layers\n","    if target_modules is None:\n","        target_modules = [\n","            \"mlp.fc1\",  # First linear layer in MLP (1024 -> 4096)\n","            \"mlp.fc2\"   # Second linear layer in MLP (4096 -> 1024)\n","        ]\n","\n","    # Step 3: Configure LoRA\n","    peft_config = LoraConfig(\n","        r=r,\n","        lora_alpha=lora_alpha,\n","        lora_dropout=lora_dropout,\n","        target_modules=target_modules,\n","        bias=\"none\",\n","        task_type=\"FEATURE_EXTRACTION\",\n","        inference_mode=False\n","    )\n","\n","    # Step 4: Apply PEFT\n","    lora_model = get_peft_model(model, peft_config)\n","\n","    # Print trainable parameters info\n","    lora_model.print_trainable_parameters()\n","\n","    return lora_model"],"metadata":{"id":"u3EaxNHcCYUK"},"execution_count":null,"outputs":[]}]}
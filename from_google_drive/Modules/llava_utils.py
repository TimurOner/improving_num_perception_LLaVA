# -*- coding: utf-8 -*-
"""llava_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1igmFdoSNl3QaquY-Muc5pdhx-mvtaJ1L
"""
# =========================
# Built-in Libraries
# =========================
import os
import gc
import re
import json
import random
import tempfile
import contextlib
import copy
from copy import deepcopy
import pickle
import itertools
from collections import Counter
from typing import Dict, List, Optional, Tuple, Union

from dataclasses import dataclass

# =========================
# Scientific Computing
# =========================
import math
import numpy as np
import pandas as pd
from scipy.stats import zscore, linregress
from scipy.spatial import ConvexHull

# =========================
# PyTorch
# =========================
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset, random_split
from torch.cuda import amp
from torch.nn.utils.rnn import pad_sequence
import torchvision
from torch.optim.lr_scheduler import CosineAnnealingLR

# =========================
# Visualization
# =========================
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib.lines import Line2D
from matplotlib.pyplot import Normalize
import matplotlib.patches as patches
import seaborn as sns

# =========================
# Machine Learning / Dimensionality Reduction
# =========================
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
from sklearn.preprocessing import normalize, StandardScaler
from sklearn.metrics import (
    confusion_matrix,
    mean_absolute_error,
    accuracy_score,
    ConfusionMatrixDisplay,
)
from sklearn.metrics.pairwise import cosine_distances, cosine_similarity

# =========================
# Transformers / Hugging Face
# =========================
from transformers import (
    AutoProcessor,
    AutoTokenizer,
    AutoModel,
    CLIPModel,
    CLIPProcessor,
    CLIPImageProcessor,
    BitsAndBytesConfig,
    AutoModelForVision2Seq,
    pipeline,
    Trainer,
    TrainingArguments,
    TrainerCallback,
    TrainerState,
    TrainerControl,
    LlavaForConditionalGeneration,
)

# =========================
# PEFT / LoRA
# =========================
from peft import (
    PeftModel,
    PeftConfig,
    get_peft_model,
    LoraConfig,
    TaskType,
    prepare_model_for_kbit_training,
)

# =========================
# UMAP
# =========================
import umap


# =========================
# Image Handling
# =========================
from PIL import Image
import cv2

# =========================
# SafeTensors
# =========================
from safetensors.torch import load_file, save_file

# =========================
# Project Utilities
# =========================
from vit_finetuning_utils import (
    PeftViTWrapper,
    load_peft_vit_classifier,
    ViTClassifier,
)

# =========================
# Progress Bar
# =========================
from tqdm import tqdm


def category_wise_inference_batched(model, dataset, tokenizer, device, batch_size=16):
    model.eval()

    # Categories always exist
    cats = dataset.obj_categories
    unique_cats = sorted(list(set(cats)))

    # Initialize results dict
    results = {cat: {"preds": [], "gt": []} for cat in unique_cats}
    results["all"] = {"preds": [], "gt": []}   # Collect all preds & gts

    N = len(dataset)

    with torch.no_grad():
        for start in tqdm(range(0, N, batch_size), desc="Batched inference"):
            end = min(start + batch_size, N)

            # Load batch items
            batch_items = [dataset[i] for i in range(start, end)]

            pixel_values = torch.stack([item[0] for item in batch_items]).to(device)
            input_ids     = torch.stack([item[1] for item in batch_items]).to(device)
            attn_mask     = torch.stack([item[2] for item in batch_items]).to(device)
            gts           = [item[4] for item in batch_items]

            # Generate predictions for the batch
            outputs = model.generate(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attn_mask,
                max_new_tokens=5,
                do_sample=False
            )

            # Decode and extract number
            decoded_batch = [tokenizer.decode(out, skip_special_tokens=True)
                             for out in outputs]
            preds = [extract_number(text) for text in decoded_batch]

            # Store both category-wise and main
            for i, pred in enumerate(preds):
                cat = cats[start + i]

                results[cat]["preds"].append(pred)
                results[cat]["gt"].append(gts[i])

                # Also append to "main"
                results["all"]["preds"].append(pred)
                results["all"]["gt"].append(gts[i])

    return results

def get_inference_ready_model(model_name, adapter_path, vit_adapt_path, quantization_config,
                               load_from_local=False, local_model_path=None, no_quantize=False):

    model_name = model_name or "llava-hf/llava-1.5-7b-hf"

    if load_from_local and local_model_path:
        load_source = local_model_path
    else:
        load_source = model_name

    processor = AutoProcessor.from_pretrained(model_name)
    processor.patch_size = 14
    processor.vision_feature_select_strategy = "default"

    if vit_adapt_path is not None:
        baseline_model = LlavaForConditionalGeneration.from_pretrained(
            load_source,
            device_map="auto",
            torch_dtype=torch.float16
        )
        baseline_vit = baseline_model.vision_tower.vision_model
        _, pefted_vit = load_peft_vit_classifier(
            baseline_vit, vit_adapt_path,
            freeze_vit=True, device='cuda', load_class_head=False
        )

        baseline_vit = pefted_vit.merge_and_unload()
        baseline_model.vision_tower.vision_model = baseline_vit

        if no_quantize == False:
            print("Quantizing merged model...")
            with tempfile.TemporaryDirectory() as tmp_dir:
                baseline_model.save_pretrained(tmp_dir)
                del baseline_model
                torch.cuda.empty_cache()
                baseline_model = LlavaForConditionalGeneration.from_pretrained(
                    tmp_dir,
                    quantization_config=quantization_config,
                    device_map="auto"
                )
    else:
        if no_quantize == False:
            baseline_model = LlavaForConditionalGeneration.from_pretrained(
                load_source,
                quantization_config=quantization_config,
                device_map="auto"
            )
        else:
            baseline_model = LlavaForConditionalGeneration.from_pretrained(
                load_source,
                device_map="auto",
                torch_dtype=torch.float16
            )

    if adapter_path is not None:
        print(f"Loading language model adapter from {adapter_path}")
        inference_ready_model = PeftModel.from_pretrained(baseline_model, adapter_path)
    else:
        inference_ready_model = baseline_model


    return processor, inference_ready_model
    
    
    
def custom_collate_fn(batch):
    pixel_values = torch.stack([item[0] for item in batch])  # shape: (B, C, H, W)

    input_ids = [item[1] for item in batch]
    attention_masks = [item[2] for item in batch]
    labels = [item[3] for item in batch]

    # Assumes tokenizer.pad_token_id == 0, which is typical
    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_mask_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)
    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)

    numerical_labels = torch.tensor([item[4] for item in batch], dtype=torch.long)
    image_paths = [item[5] for item in batch]

    return (
       pixel_values,
       input_ids_padded,
       attention_mask_padded,
       labels_padded,
       numerical_labels,
       image_paths)   
def free_vit_models(model_list):
    
    
    for model, name in model_list:
        print(f"Deleting model: {name}")
        del model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("CUDA cache cleared.")    
def sanity_check_llava_model(model, train_dataset, processor, device):
    ex_prompt = (
        "USER: <image>\n"
        "Count the total number of distinct objects in the photo. "
        "Answer only with the final count as a numeral.\n"
        "ASSISTANT:"
    )

    n = len(train_dataset)
    rand_id = random.randint(0, n - 1)
    rand_sample = train_dataset[rand_id]

    image_path = rand_sample[-1]
    image_PIL = Image.open(image_path).convert("RGB")
    true_num = rand_sample[-2]

    inputs = processor(text=ex_prompt, images=image_PIL, return_tensors="pt").to(device)

    model.eval()
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=5,
            do_sample=False
        )

    decoded = processor.decode(output[0], skip_special_tokens=True)

    print(f"Image: {image_path}")
    print(f"True count: {true_num}")
    print(f"Model output: {decoded}")


def check_lora_weight_diff(baseline_model, finetuned_model, atol=1e-6):
    modules_to_check = [
        "model.multi_modal_projector.linear_1",
        "model.multi_modal_projector.linear_2",
    ]

    baseline_params = dict(baseline_model.named_parameters())
    finetuned_params = dict(finetuned_model.named_parameters())

    for module_name in modules_to_check:
        if f"{module_name}.weight" not in baseline_params:
            print(f"‚ö†Ô∏è Skipping {module_name}, not found in baseline model")
            continue
        if f"{module_name}.weight" not in finetuned_params:
            print(f"‚ö†Ô∏è Skipping {module_name}, not found in finetuned model")
            continue

        baseline_weight = baseline_params[f"{module_name}.weight"].detach().cpu().float()
        finetuned_weight = finetuned_params[f"{module_name}.weight"].detach().cpu().float()

        diff = (finetuned_weight - baseline_weight).abs()
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()

        print(f"üîç Checking {module_name}:")
        print(f"   Max abs diff:  {max_diff:.6e}")
        print(f"   Mean abs diff: {mean_diff:.6e}")
        if torch.allclose(baseline_weight, finetuned_weight, atol=atol):
            print("   ‚úÖ No significant changes (within tolerance)\n")
        else:
            print("   ‚ö†Ô∏è Weights changed (LoRA seems applied)\n")
            

def prepare_peft_model(llavalora_config, training_config):
    model_name = "llava-hf/llava-1.5-7b-hf"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    image_processor = AutoProcessor.from_pretrained(model_name).image_processor

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )

    llava_processor, base_model = get_inference_ready_model(
        model_name, None, training_config.custom_vit_path, quantization_config
    )
    device = base_model.device

    peft_lora_config = LoraConfig(
        r=llavalora_config.r,
        lora_alpha=llavalora_config.lora_alpha,
        target_modules=llavalora_config.target_modules,
        lora_dropout=llavalora_config.lora_dropout,
        bias=llavalora_config.bias,
        task_type=llavalora_config.task_type
    )

    base_model_copy = copy.deepcopy(base_model)
    base_model_copy = prepare_model_for_kbit_training(base_model_copy)
    peft_model = get_peft_model(base_model_copy, peft_lora_config)

    return tokenizer, llava_processor, peft_model, device
def simple_manual_inference(model_untuned, model_tuned, tokenizer, dataset, device):
    model_untuned.eval()
    model_tuned.eval()

    preds_untuned = []
    preds_tuned = []
    ground_truths = []

    
    with torch.no_grad():
        for i in range(len(dataset)):
            item = dataset[i]
            pixel_values = item[0].unsqueeze(0).to(device)
            attn_mask = item[2].unsqueeze(0).to(device)
            input_ids = item[1].unsqueeze(0).to(device)
            gt = item[4]



            outputs_untuned = model_untuned.generate(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attn_mask,
                max_new_tokens=5,
                do_sample=False
            )

            outputs_tuned = model_tuned.generate(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attn_mask,
                max_new_tokens=5,
                do_sample=False
            )

            decoded_untuned = tokenizer.decode(outputs_untuned[0], skip_special_tokens=True)
            decoded_tuned = tokenizer.decode(outputs_tuned[0], skip_special_tokens=True)


            pred_untuned = extract_number(decoded_untuned)
            pred_tuned = extract_number(decoded_tuned)



            preds_untuned.append(pred_untuned)
            preds_tuned.append(pred_tuned)
            ground_truths.append(gt)

            print(f"Sample {i+1}/{len(dataset)} | GT: {gt} | Untuned: {pred_untuned} | Tuned: {pred_tuned}")

    return preds_untuned, preds_tuned, ground_truths
    
def extract_number(response):
    """Extract integer from model response"""
    import re
    # Find all numbers in the response
    numbers = re.findall(r'\d+', response.split("ASSISTANT:")[-1] if "ASSISTANT:" in response else response)
    return int(numbers[-1]) if numbers else 0




def train_model(
    model,
    train_loader,
    val_loader,
    test_loader,
    optimizer,
    device,
    epochs=10,
    early_stop_tolerance=3,
    early_stop_perc_thr=0.01,
    use_mixed_precision=False
):
    train_loss_log = []
    val_loss_log = []
    best_loss = float('inf')
    epochs_without_improvement = 0
    
  
    model_initial  =  deepcopy(model)
    scaler = amp.GradScaler() if use_mixed_precision else None

    # Fixed: Single scheduler for all parameter groups
    initial_lr = optimizer.param_groups[0]['lr']  # Use first group's LR as reference
    min_lr = initial_lr / 10
    scheduler = CosineAnnealingLR(
        optimizer,
        T_max=epochs,
        eta_min=min_lr
    )
     

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0

        with tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}") as pbar:
            for pixel_values, input_ids, attn, labels, *_ in pbar:
                pixel_values = pixel_values.to(device)
                input_ids = input_ids.to(device)
                attn = attn.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()

                if use_mixed_precision:
                    with amp.autocast():
                        outputs = model(
                            input_ids=input_ids,
                            attention_mask=attn,
                            pixel_values=pixel_values,
                            labels=labels,
                        )
                        loss = outputs.loss
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(
                        input_ids=input_ids,
                        attention_mask=attn,
                        pixel_values=pixel_values,
                        labels=labels,
                    )
                    loss = outputs.loss
                    loss.backward()
                    optimizer.step()

                running_loss += loss.item()
                pbar.set_postfix(loss=running_loss / (pbar.n + 1))

        # Step scheduler once per epoch
        scheduler.step()

        train_loss = running_loss / len(train_loader)
        train_loss_log.append(train_loss)

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for pixel_values, input_ids, attn, labels, *_ in val_loader:
                pixel_values = pixel_values.to(device)
                input_ids = input_ids.to(device)
                attn = attn.to(device)
                labels = labels.to(device)

                context = amp.autocast() if use_mixed_precision else contextlib.nullcontext()
                with context:
                    val_loss += model(
                        input_ids=input_ids,
                        attention_mask=attn,
                        pixel_values=pixel_values,
                        labels=labels,
                    ).loss.item()

        val_loss /= len(val_loader)
        val_loss_log.append(val_loss)



        print(f"Epoch {epoch+1}: Train Loss={train_loss:.4f} | Val Loss={val_loss:.4f}")
        # Improved early stopping logic
        if val_loss < best_loss:
            # Calculate relative improvement
            if best_loss != float('inf'):
                improvement = (best_loss - val_loss)
            else:
                improvement = float('inf')  # First epoch always counts as improvement

            if improvement >= early_stop_perc_thr or best_loss == float('inf'):
                best_loss = val_loss
                epochs_without_improvement = 0
                print(f"‚Üí New best validation loss: {val_loss:.4f} (improvement: {improvement:.4f})")
            else:
                epochs_without_improvement += 1
                print(f"‚Üí Early stopping counter: {epochs_without_improvement}/{early_stop_tolerance} (improvement: {improvement:.4f})")
        else:
            epochs_without_improvement += 1
            print(f"‚Üí Early stopping counter: {epochs_without_improvement}/{early_stop_tolerance} (no improvement)")

        if epochs_without_improvement >= early_stop_tolerance:
            print(f"Early stopping triggered at epoch {epoch+1}.")
            break

    return model, train_loss_log, val_loss_log


@dataclass
class LLaVALoRAConfig:
    r: int
    lora_alpha: float
    target_modules: List[str]
    lora_dropout: float
    bias: str
    task_type: str

@dataclass
class TrainingConfig:
    learning_rate: float
    weight_decay: float
    total_epochs: int
    batch_size: int
    early_stop_tolerance: int
    early_stop_trig: float
    custom_vit_path: Optional[str] = None

@dataclass
class DatasetConfig:
    train_dataset_name: str
    val_dataset_name: str
    test_dataset_name: str
    train_nums: List[int]
    val_nums: List[int]
    test_nums: List[int]
    train_objs: List[str]
    val_objs: List[str]
    test_objs: List[str]
    num_augment_train: int
    num_augment_factor: int
    extra_basic_aug: bool
    results_save_path: str
    adapter_save_path: str
    prompt_inference: bool

def load_config(config_path: str):
    """Load configuration from JSON and unpack into structured dataclasses"""
    with open(config_path, 'r') as f:
        cfg = json.load(f)

    # Create dataclass instances
    lora_config = LLaVALoRAConfig(
        r=cfg['r'],
        lora_alpha=cfg['lora_alpha'],
        target_modules=cfg['target_modules'],
        lora_dropout=cfg['lora_dropout'],
        bias=cfg['bias'],
        task_type=cfg['task_type']
    )

    training_config = TrainingConfig(
        learning_rate=cfg['learning_rate'],
        weight_decay = cfg['weight_decay'],
        total_epochs=cfg['total_epochs'],
        batch_size=cfg['batch_size'],
        early_stop_tolerance=cfg['early_stop_tolerance'],
        early_stop_trig=cfg['early_stop_trig'],
        custom_vit_path=cfg.get('custom_vit_path', None)
    )

    dataset_config = DatasetConfig(
        train_dataset_name=cfg['train_dataset_name'],
        val_dataset_name=cfg['val_dataset_name'],
        test_dataset_name=cfg['test_dataset_name'],
        train_nums=cfg['train_nums'],
        val_nums=cfg['val_nums'],
        test_nums=cfg['test_nums'],
        train_objs=cfg['train_objs'],
        val_objs=cfg['val_objs'],
        test_objs=cfg['test_objs'],
        num_augment_train=cfg['num_augment_train'],
        num_augment_factor=cfg['num_augment_factor'],
        extra_basic_aug=cfg['extra_basic_aug'],
        results_save_path=cfg['preds_save_path'],
        adapter_save_path=cfg['adapter_save_path'],
        prompt_inference=cfg['prompt_inference']
    )

    # Print configuration summary
    print("=" * 60)
    print("CONFIGURATION SUMMARY")
    print("=" * 60)

    print(f"\nüìä DATASET & TRAINING:")
    print(f"  Train Dataset: {dataset_config.train_dataset_name}")
    print(f"  Val Dataset: {dataset_config.val_dataset_name}")
    print(f"  Test Dataset: {dataset_config.test_dataset_name}")
    print(f"  Learning Rate: {training_config.learning_rate}")
    print(f"  Total Epochs: {training_config.total_epochs}")
    print(f"  Batch Size: {training_config.batch_size}")

    print(f"\nüíæ SAVE PATHS:")
    print(f"  Predictions Save Path: {dataset_config.results_save_path}")
    print(f"  Adapter Save Path: {dataset_config.adapter_save_path}")
    print(f"  Prompt Inference: {dataset_config.prompt_inference}")

    print(f"\nüî¢ DATA SPLITS:")
    print(f"  Train Numbers: {dataset_config.train_nums} (Total: {len(dataset_config.train_nums)})")
    print(f"  Val Numbers: {dataset_config.val_nums} (Total: {len(dataset_config.val_nums)})")
    print(f"  Test Numbers: {dataset_config.test_nums} (Total: {len(dataset_config.test_nums)})")

    print(f"\nüß© DATA AUGMENTATION:")
    print(f"  Augmentation Enabled: {dataset_config.num_augment_train}")
    print(f"  Augmentation Factor: {dataset_config.num_augment_factor}")
    print(f"  Extra Basic Aug: {dataset_config.extra_basic_aug}")

    print(f"\nüéØ OBJECT CATEGORIES:")
    print(f"  Train Objects: {dataset_config.train_objs} (Total: {len(dataset_config.train_objs)})")
    print(f"  Val Objects: {dataset_config.val_objs} (Total: {len(dataset_config.val_objs)})")
    print(f"  Test Objects: {dataset_config.test_objs} (Total: {len(dataset_config.test_objs)})")

    print(f"\nüîß LORA CONFIGURATION:")
    print(f"  Rank (r): {lora_config.r}")
    print(f"  Alpha: {lora_config.lora_alpha}")
    print(f"  Dropout: {lora_config.lora_dropout}")
    print(f"  Bias: {lora_config.bias}")
    print(f"  Task Type: {lora_config.task_type}")
    print(f"  Target Modules: {', '.join(lora_config.target_modules)}")

    print(f"\n‚èπÔ∏è EARLY STOPPING:")
    print(f"  Tolerance: {training_config.early_stop_tolerance} epochs")
    print(f"  Threshold: {training_config.early_stop_trig}")

    print("=" * 60)

    return dataset_config, training_config, lora_config



def sanity_check_dataset(dataset, model, tokenizer, device):
    import random
    
    n = len(dataset)
    rand_id = random.randint(0, n - 1)
    
    pixel_values, input_ids, attention_mask, labels, original_numerical_label, image_path = dataset[rand_id]
    
    print(f"\n=== Dataset Sanity Check ===")
    print(f"Sample index: {rand_id}/{n}")
    print(f"Image path: {image_path}")
    print(f"Original numerical label: {original_numerical_label}")
    
    print(f"\nPixel values shape: {pixel_values.shape}")
    print(f"Pixel values: min={pixel_values.min():.3f}, max={pixel_values.max():.3f}, mean={pixel_values.mean():.3f}")
    
    print(f"\nInput IDs shape: {input_ids.shape}")
    print(f"Input IDs: {input_ids[:30]}")
    print(f"Decoded input: {tokenizer.decode(input_ids, skip_special_tokens=True)}")
    
    print(f"\nAttention mask shape: {attention_mask.shape}")
    print(f"Attention mask sum: {attention_mask.sum()}")
    
    print(f"\nLabels shape: {labels.shape}")
    print(f"Labels (non -100): {labels[labels != -100]}")
    print(f"Decoded labels: {tokenizer.decode(labels[labels != -100], skip_special_tokens=True)}")
    
    print(f"\n=== Testing with model ===")
    model.eval()
    with torch.no_grad():
        pixel_values_batch = pixel_values.unsqueeze(0).to(device)
        input_ids_batch = input_ids.unsqueeze(0).to(device)
        attention_mask_batch = attention_mask.unsqueeze(0).to(device)
        
        output = model.generate(
            pixel_values=pixel_values_batch,
            input_ids=input_ids_batch,
            attention_mask=attention_mask_batch,
            max_new_tokens=5,
            do_sample=False
        )
        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)
        print(f"Model output: {decoded_output}")
        
        
def inference_yes_no(model, dataset, tokenizer, device, batch_size=16):

    # Get categories from dataset (works for both base and wrapper datasets)
    cats = dataset.obj_categories
    unique_cats = sorted(list(set(cats)))

    category_results = {}

    for cat in tqdm.tqdm(unique_cats, desc="Categories"):
        indices_in_cat = [i for i, c in enumerate(cats) if c == cat]

        all_preds = []
        all_gts = []
        all_true_counts = []

        for start_idx in tqdm.tqdm(range(0, len(indices_in_cat), batch_size),
                                    desc="Batched inference", leave=False):
            end_idx = min(start_idx + batch_size, len(indices_in_cat))
            batch_indices = indices_in_cat[start_idx:end_idx]
            batch_items = [dataset[i] for i in batch_indices]

            # Extract components
            pixel_values_list = [item[0] for item in batch_items]
            input_ids_list = [item[1] for item in batch_items]
            attn_mask_list = [item[2] for item in batch_items]
            gts = [item[4] for item in batch_items]
            image_paths = [item[5] for item in batch_items]

            # Extract true counts from image paths
            true_counts = []
            for path in image_paths:
                img_name = os.path.basename(path)
                parts = img_name.split('_')
                for part in parts:
                    if part.isdigit() and int(part) < 11:
                        true_counts.append(int(part))
                        break

            # Pad sequences to max length in batch
            max_len = max(ids.shape[0] for ids in input_ids_list)
            padded_input_ids = []
            padded_attn_masks = []

            for input_ids, attn_mask in zip(input_ids_list, attn_mask_list):
                pad_len = max_len - input_ids.shape[0]
                if pad_len > 0:
                    # Left-pad for decoder-only models
                    input_ids = torch.cat([
                        torch.full((pad_len,), tokenizer.pad_token_id, dtype=input_ids.dtype),
                        input_ids
                    ])
                    attn_mask = torch.cat([
                        torch.zeros(pad_len, dtype=attn_mask.dtype),
                        attn_mask
                    ])
                padded_input_ids.append(input_ids)
                padded_attn_masks.append(attn_mask)

            # Stack and move to device
            pixel_values = torch.stack(pixel_values_list).to(device)
            input_ids = torch.stack(padded_input_ids).to(device)
            attn_mask = torch.stack(padded_attn_masks).to(device)

            # Generate predictions
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=input_ids,
                    attention_mask=attn_mask,
                    pixel_values=pixel_values,
                    max_new_tokens=10,
                    do_sample=False
                )

            # Decode only the generated tokens and clean predictions
            for i, output in enumerate(outputs):
                  pred_text = tokenizer.decode(output[max_len:],skip_special_tokens=True).strip().lower().lstrip(': ')
                  all_preds.append(pred_text)
                  all_gts.append(gts[i])
                  all_true_counts.append(true_counts[i])
        category_results[cat] = {
            'predictions': all_preds,
            'ground_truths': all_gts,
            'true_counts': all_true_counts
        }

    return category_results
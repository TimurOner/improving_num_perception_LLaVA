# -*- coding: utf-8 -*-
"""vit_finetuning_utils.ipynb

Automatically generated by Colab.

Original file is located at:
    https://colab.research.google.com/drive/1kzPeTXar1LQ3oOv8-REjKY-3NZNwczIk
"""

# === Standard Library ===
import os,json
import math
import pickle
from copy import deepcopy
from dataclasses import dataclass
from typing import List

# === PyTorch ===
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
from torchvision import transforms

# === PyTorch AMP ===
from torch.cuda.amp import GradScaler, autocast

# === Transformers / PEFT ===
from transformers import AutoProcessor
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel

# === NumPy / SciPy / Data Processing ===
import numpy as np
from sklearn.preprocessing import normalize as sk_normalize
from sklearn.decomposition import PCA, TruncatedSVD

# === Dimensionality Reduction / Visualization ===
from sklearn.manifold import TSNE
import umap

# === Visualization ===
import matplotlib.pyplot as plt
import tqdm


model_name = "llava-hf/llava-1.5-7b-hf"
processor = AutoProcessor.from_pretrained(model_name)
processor.patch_size = 14
processor.vision_feature_select_strategy = "default"


from val_test_funs import plot_preds_vs_gts



def batch_predict_plot_save(
    model_list,
    dataloader_list,
    device='cuda',
    save_path=None
):
    if save_path:
        os.makedirs(save_path, exist_ok=True)

    all_preds_labels = {}

    for model, trained_on in model_list:
        for dataloader, dataset_name in dataloader_list:
            base_filename = f"{dataset_name}_trained_on_{trained_on}".replace(" ", "_")
            pred_file = os.path.join(save_path, base_filename + "_preds.pkl") if save_path else None
            label_file = os.path.join(save_path, base_filename + "_labels.pkl") if save_path else None

            if pred_file and os.path.exists(pred_file) and os.path.exists(label_file):
                with open(pred_file, 'rb') as f:
                    preds = pickle.load(f)
                with open(label_file, 'rb') as f:
                    labels = pickle.load(f)
            else:

                preds, labels = predict_vit(model, dataloader, device, mixed_prec=True, on_cats=False)
                preds,labels = preds +1,labels+1
                if save_path:
                    with open(pred_file, 'wb') as f:
                        pickle.dump(preds, f)
                    with open(label_file, 'wb') as f:
                        pickle.dump(labels, f)

            all_preds_labels[(trained_on, dataset_name)] = (preds, labels)

            title = f"{dataset_name} (Trained on {trained_on})"
            plot_file = os.path.join(save_path, base_filename + ".png") if save_path else None
            plot_preds_vs_gts(preds, labels, title=title, save_path=plot_file)

    return all_preds_labels
    
    
    
# === Partial Unfreezing ===
def partially_unfreeze_vit(model, unfreeze_last_n_layers=4):
    for param in model.parameters():
        param.requires_grad = False

    for layer in model.encoder.layers[-unfreeze_last_n_layers:]:
        for param in layer.parameters():
            if param.is_floating_point():
                param.requires_grad = True

    total_params = sum(p.numel() for p in model.parameters())
    frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)
    unfrozen_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"Total parameters: {total_params}")
    print(f"Frozen parameters: {frozen_params}")
    print(f"Unfrozen parameters: {unfrozen_params}")


# === LoRA (PEFT) Setup ===
def setup_peft_vit_mlp(model_in, r=8, lora_alpha=16, lora_dropout=0.1, target_modules=None, use_gradient_checkpointing=True):
    model = deepcopy(model_in)
    del model_in
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)

    if target_modules is None:
        target_modules = ["mlp.fc1", "mlp.fc2"]

    peft_config = LoraConfig(
        r=r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        bias="none",
        task_type="FEATURE_EXTRACTION",
        inference_mode=False
    )

    lora_model = get_peft_model(model, peft_config)
    lora_model.print_trainable_parameters()
    return lora_model


# === Vision Tower Extractor ===
def get_vit_and_print_config(llava_model):
    vit = llava_model.model.vision_tower.vision_model
    print(vit)
    return vit


# === ViT Classifier ===

class ViTClassifier(nn.Module):
    def __init__(self, vit_model, num_classes, mode="cls", svd=False, svd_dim=256, freeze_vit=False):
        """
        mode: 'cls', 'patch'
        """
        super(ViTClassifier, self).__init__()
        self.vit = vit_model
        self.mode = mode
        self.svd = svd
        self.svd_dim = svd_dim
        self.freeze_vit = freeze_vit


        if freeze_vit:
            for p in self.vit.parameters():
                p.requires_grad = False

        base_dim = vit_model.embeddings.patch_embedding.out_channels
        self.num_patches = vit_model.embeddings.position_embedding.num_embeddings - 1

        if mode == "cls":
            self.feature_dim = base_dim
        elif mode == "patch":
            self.feature_dim = base_dim
            self.attn = nn.MultiheadAttention(embed_dim=base_dim, num_heads=1, batch_first=True)
        else:
            raise ValueError(f"Unknown mode: {mode}")

        classifier_input_dim =1024


        self.classifier = nn.Linear(classifier_input_dim, num_classes)
        self.attention_scores = None

        print(f"üîß ViTClassifier init: mode={mode}, svd={svd}, feature_dim={self.feature_dim}, classifier_input={classifier_input_dim}")

    def fit_svd(self, train_features):
        if not self.svd:
            raise ValueError("SVD not enabled")

        print(f"üìê Fitting NumPy SVD on features: {train_features.shape}")
        features_np = train_features
        U, S, Vt = np.linalg.svd(features_np, full_matrices=False)
        variance_retained = (S[:self.svd_dim]**2).sum() / (S**2).sum() * 100
        print(f'{variance_retained:.2f}% of variance is retained.')
        Vt = Vt[:self.svd_dim].T  # [orig_dim, svd_dim]
        self.svd_projection = nn.Parameter(
            torch.from_numpy(Vt).to(next(self.parameters()).device), requires_grad=False
        )
        print(f"‚úÖ SVD projection: {features_np.shape[1]} ‚Üí {self.svd_dim}")

    def forward(self, x):
        if self.freeze_vit:
            if self.mode in [ "all"]:
                # x is already precomputed features
                # apply attention if mode is patch/all
                attn_out, _ = self.attn(x, x, x)
                features = attn_out.mean(dim=1)
            else:
                features = x
        else:
            with torch.no_grad() if not any(p.requires_grad for p in self.vit.parameters()) else torch.enable_grad():
                outputs = self.vit(x)
            hidden = outputs.last_hidden_state

            if self.mode == "cls":
                features = hidden[:, 0, :]
            elif self.mode == "patch":
                # attn_out, _ = self.attn(hidden[:, 1:, :], hidden[:, 1:, :], hidden[:, 1:, :])
                # features = attn_out.mean(dim=1)
                features = hidden[:, 1:, :].mean(dim=1)
            else:
                raise ValueError(f"Unknown mode: {self.mode}")

        if self.svd:
            if self.svd_projection is None:
                raise RuntimeError("Call fit_svd() before forward")
            features = features @ self.svd_projection

        return self.classifier(features)
# === PEFT Wrapper ===
class PeftViTWrapper(nn.Module):
    def __init__(self, vit_peft):
        super().__init__()
        self.vit_peft = vit_peft

    def __getattr__(self, name):
        try:
            return super().__getattr__(name)
        except AttributeError:
            return getattr(self.vit_peft, name)

    def forward(self, x, *args, **kwargs):
        return self.vit_peft.base_model.model(pixel_values=x)


# === Target Module Builder ===
def target_module_list_creator(module_types_list, target_levels=None):
    target_modules = []

    for module_type in module_types_list:
        if module_type == 'MLP':
            if target_levels is None:
                target_modules.extend(['mlp.fc1', 'mlp.fc2'])
            else:
                for level in target_levels:
                    target_modules.extend([
                        f'encoder.layers.{level}.mlp.fc1',
                        f'encoder.layers.{level}.mlp.fc2'
                    ])

        elif module_type == 'att_q':
            if target_levels is None:
                target_modules.append('self_attn.q_proj')
            else:
                for level in target_levels:
                    target_modules.append(f'encoder.layers.{level}.self_attn.q_proj')

        elif module_type == 'att_k':
            if target_levels is None:
                target_modules.append('self_attn.k_proj')
            else:
                for level in target_levels:
                    target_modules.append(f'encoder.layers.{level}.self_attn.k_proj')

        elif module_type == 'att_v':
            if target_levels is None:
                target_modules.append('self_attn.v_proj')
            else:
                for level in target_levels:
                    target_modules.append(f'encoder.layers.{level}.self_attn.v_proj')

    return target_modules


# === Print Model Modules ===
def print_all_module_names(model, trainable_only=False):
    print("\n=== All Module Names in Model ===")
    for name, module in model.named_modules():
        if trainable_only:
            has_trainable_params = any(p.requires_grad for p in module.parameters(recurse=False))
            if not has_trainable_params:
                continue
        print(name)
    print("=================================\n")

# === === === === === === === === === === === ===# 
# === Training and Training Helper Functions === # 
# === === === === === === === === === === === ===# 

def train_vit_classifier(
    model,
    train_loader,
    val_loader,
    device=None,
    on_cats=False,
    num_epochs=10,
    lr=1e-4,
    weight_decay=0.01,
    mixed_prec_train=False,
    warmup_steps=500,
    patience=3
):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    total_steps = num_epochs * len(train_loader)

    def lr_lambda(current_step):
        if current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return 0.5 * (1.0 + math.cos(math.pi * progress))

    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    scaler = torch.amp.GradScaler(enabled=mixed_prec_train)

    train_losses, train_accs, val_losses, val_accs = [], [], [], []

    global_step = 0

    for epoch in range(1, num_epochs + 1):
        # --- Training ---
        model.train()
        train_loss, correct_train, total_train = 0.0, 0, 0

        loop = tqdm.tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs} [Train]", leave=False)
        for batch_idx, batch in enumerate(loop):
            if on_cats:
                images, _, _, cat_labels = batch
                labels = cat_labels
            else:
                images, labels, _, _ = batch

            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            with torch.amp.autocast('cuda', enabled=mixed_prec_train):
                logits = model(images)
                loss = criterion(logits, labels)

            if mixed_prec_train:
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()

            scheduler.step()
            global_step += 1

            train_loss += loss.item() * images.size(0)
            preds = logits.argmax(dim=1)
            correct_train += (preds == labels).sum().item()
            total_train += labels.size(0)

            loop.set_postfix(loss=train_loss / total_train, acc=correct_train / total_train)

        train_loss /= total_train
        train_acc = correct_train / total_train

        # --- Validation ---
        model.eval()
        val_loss, correct_val, total_val = 0.0, 0, 0

        loop_val = tqdm.tqdm(val_loader, desc=f"Epoch {epoch}/{num_epochs} [Val]", leave=False)
        with torch.no_grad(), torch.amp.autocast('cuda', enabled=mixed_prec_train):
            for batch in loop_val:
                if on_cats:
                    images, _, _, cat_labels = batch
                    labels = cat_labels
                else:
                    images, labels, _, _ = batch

                images, labels = images.to(device), labels.to(device)
                logits = model(images)
                loss = criterion(logits, labels)

                val_loss += loss.item() * images.size(0)
                preds = logits.argmax(dim=1)
                correct_val += (preds == labels).sum().item()
                total_val += labels.size(0)

                loop_val.set_postfix(loss=val_loss / total_val, acc=correct_val / total_val)

        val_loss /= total_val
        val_acc = correct_val / total_val

        # --- Save metrics ---
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        val_losses.append(val_loss)
        val_accs.append(val_acc)

        # Print epoch summary
        print(f"Epoch {epoch}/{num_epochs} => "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    return train_losses, train_accs, val_losses, val_accs
    
    
def train_ll_classifier(pefted_vit_without_classifier, train_dataloader, val_dataloader, test_dataloader, device, **h_params):
    pefted_vit_copy = deepcopy(pefted_vit_without_classifier)
    num_classes = 3 if h_params.get("on_cats", False) else 10

    mode = h_params.get("mode", "cls")
    use_svd = h_params.get("use_svd", False)
    svd_dim = h_params.get("svd_dim", 128)

    h_params_train = {
        "num_epochs": h_params.get("num_epochs"),
        "lr": h_params.get("lr"),
        "weight_decay": h_params.get("weight_decay"),
        "mixed_prec_train": h_params.get("mixed_prec_train"),
        "on_cats": h_params.get("on_cats"),
        "warmup_steps": h_params.get("warmup_steps"),
        "patience": h_params.get("patience"),
    }

    pefted_vit_with_classifier = ViTClassifier(
        PeftViTWrapper(pefted_vit_copy),
        num_classes=num_classes,
        freeze_vit=True,
        mode=mode,
        svd=use_svd,
        svd_dim=svd_dim,
    ).to(device)

    if not any(p.requires_grad for p in pefted_vit_with_classifier.vit.parameters()):
        print("‚ö° ViT is frozen ‚Äî caching features before classifier training...")

        def _extract_feature_matrix(dataloader):
            pefted_vit_with_classifier.eval()
            feature_matrix_list = []
            print(f"üîç Extracting features with mode={mode}...")
            with torch.no_grad(), torch.amp.autocast("cuda", enabled=h_params.get("mixed_prec_train", False)):
                for batch in tqdm.tqdm(dataloader, desc="Building SVD feature matrix"):
                    if h_params.get("on_cats", False):
                        images, _, _, _ = batch
                    else:
                        images, _, _, _ = batch
                    images = images.to(device)
                    outputs = pefted_vit_with_classifier.vit(images)
                    hidden_states = outputs.last_hidden_state
                    if mode == "cls":
                        features = hidden_states[:, 0, :].cpu().numpy()
                    elif mode == "patch":
                        features = hidden_states[:, 1:, :].cpu().numpy()
                    else:
                        raise ValueError(f"Unknown mode: {mode}")
                    norms = np.linalg.norm(features, axis=1, keepdims=True) + 1e-8
                    features = features / norms
                    feature_matrix_list.append(features)
            feature_matrix = np.concatenate(feature_matrix_list, axis=0)
            print(f"‚úÖ The SVD Matrix Dimensions: {feature_matrix.shape}, Size: {feature_matrix.nbytes / (1024 ** 2):.2f} MB")
            return feature_matrix

        def _extract_features(dataloader, name):
            pefted_vit_with_classifier.eval()
            all_features, all_labels = [], []
            print(f"üîç Extracting features with mode={mode}...")
            with torch.no_grad(), torch.amp.autocast("cuda", enabled=h_params.get("mixed_prec_train", False)):
                for batch in tqdm.tqdm(dataloader, desc=f"Caching {name} features"):
                    if h_params.get("on_cats", False):
                        images, _, _, cat_labels = batch
                        labels = cat_labels
                    else:
                        images, labels, _, _ = batch
                    images, labels = images.to(device), labels.to(device)
                    outputs = pefted_vit_with_classifier.vit(images)
                    hidden_states = outputs.last_hidden_state
                    if mode == "cls":
                        features = hidden_states[:, 0, :]
                    elif mode == "patch":
                        features = hidden_states[:, 1:, :]
                    else:
                        raise ValueError(f"Unknown mode: {mode}")
                    features = features / (features.norm(dim=1, keepdim=True) + 1e-8)
                    all_features.append(features.cpu())
                    all_labels.append(labels.cpu())
            features = torch.cat(all_features)
            labels = torch.cat(all_labels)
            print(f"‚úÖ Cached {name} features: {features.shape}")
            return features, labels

        if use_svd:
            if mode in ["patch"]:
                raise ValueError(f"SVD not supported for mode={mode}")
            else:
                feature_matrix = _extract_feature_matrix(train_dataloader)
                pefted_vit_with_classifier.fit_svd(feature_matrix)
                print("‚úÖ SVD fitted.")

        train_features, train_labels = _extract_features(train_dataloader, "train")
        val_features, val_labels = _extract_features(val_dataloader, "val")
        test_features, test_labels = _extract_features(test_dataloader, "test")

        def _wrap_dataset(features, labels, on_cats):
            dummy = torch.zeros(len(features))
            if on_cats:
                return torch.utils.data.TensorDataset(features, dummy, dummy, labels)
            else:
                return torch.utils.data.TensorDataset(features, labels, dummy, dummy)

        train_ds = _wrap_dataset(train_features, train_labels, h_params.get("on_cats", False))
        val_ds = _wrap_dataset(val_features, val_labels, h_params.get("on_cats", False))
        test_ds = _wrap_dataset(test_features, test_labels, h_params.get("on_cats", False))

        train_dataloader = DataLoader(train_ds, batch_size=train_dataloader.batch_size, shuffle=True)
        val_dataloader = DataLoader(val_ds, batch_size=val_dataloader.batch_size, shuffle=False)
        test_dataloader = DataLoader(test_ds, batch_size=test_dataloader.batch_size, shuffle=False)

        print("üîÅ Using cached embeddings for classifier training.")

    train_losses, train_accs, val_losses, val_accs = train_vit_classifier(
        pefted_vit_with_classifier, train_dataloader, val_dataloader, device, **h_params_train
    )

    return pefted_vit_with_classifier, train_losses, train_accs, val_losses, val_accs, test_dataloader

def unnormalize(img, mean, std):
    # img: tensor (C,H,W)
    mean = torch.tensor(mean).view(3,1,1)
    std = torch.tensor(std).view(3,1,1)
    return img * std + mean

def visualize_vit_transform(dataset, indices, vit_transform, mean, std):
    plt.figure(figsize=(10, len(indices)*3))

    for i, idx in enumerate(indices):
        orig_img = dataset[idx][0]
        transformed_img = vit_transform(orig_img)

        # Unnormalize for display
        display_img = unnormalize(transformed_img, mean, std)

        plt.subplot(len(indices), 2, 2*i + 1)
        plt.imshow(orig_img.permute(1,2,0))
        plt.axis("off")

        plt.subplot(len(indices), 2, 2*i + 2)
        plt.imshow(display_img.permute(1,2,0).clamp(0,1))
        plt.axis("off")

    plt.tight_layout()
    plt.show()

    
def evaluate_on_test_set(model, test_loader, device, on_cats=False, mixed_prec_train=False):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    test_loss, correct_test, total_test = 0.0, 0, 0

    with torch.no_grad(), torch.amp.autocast('cuda', enabled=mixed_prec_train):
        for batch in test_loader:
            if on_cats:
                images, _, _, cat_labels = batch
                labels = cat_labels
            else:
                images, labels, _, _ = batch

            images, labels = images.to(device), labels.to(device)
            logits = model(images)
            loss = criterion(logits, labels)

            test_loss += loss.item() * images.size(0)
            preds = logits.argmax(dim=1)
            correct_test += (preds == labels).sum().item()
            total_test += labels.size(0)

    test_loss /= total_test
    test_acc = correct_test / total_test
    return test_loss, test_acc
    
    
  
### --- ### --- ### --- ### --- ### --- ###### --- ### --- ### --- ### --- ### --- ###
##              EMBEDDING EXTRACTION TOOLS FOR SUPERVISED TUNING ViT
### --- ### --- ### --- ### --- ### --- ###### --- ### --- ### --- ### --- ### --- ###
def collect_unprojected_embeddings(
    dataloader,
    vision_tower,
    device,
    dim_red_method='svd',
    normalize=True,
    pre_reduction=False,
    pre_reduction_dim=400,
    SEED=42,
    mode='cls'
):
    all_embeddings, numerosities, obj_types = [], [], []

    for i, (img_tensor, label, cat_name, cat_label) in enumerate(
        tqdm.tqdm(dataloader, desc="Extracting unproj embeddings")
    ):
        img_tensor = img_tensor.unsqueeze(0).to(device)
        unproj_embeds = get_unprojected_visual_embeddings(img_tensor, vision_tower, device)

        if mode == 'cls':
            unproj_embeds = unproj_embeds[0:1, :]
        elif mode == 'patches':
            unproj_embeds = unproj_embeds[1:, :].mean(axis=0, keepdims=True)
        else:
            raise ValueError(f"Unknown mode: {mode}. Choose 'cls' or 'patches'.")

        all_embeddings.append(unproj_embeds)
        numerosities.append(label + 1)
        obj_types.append(cat_name)

    all_embeddings = np.vstack(all_embeddings)
    numerosities = np.array(numerosities)
    obj_types = np.array(obj_types)

    print("Collected embeddings:", all_embeddings.shape)

    embeddings_2d = process_visual_unproj_embeddings(
        img_embeddings=all_embeddings,
        max_count=len(obj_types),
        dim_red_method=dim_red_method,
        pre_reduction=pre_reduction,
        SEED=SEED,
        pre_reduction_dim=pre_reduction_dim,
        normalize=normalize
    )

    print("Final reduced embedding shape:", embeddings_2d.shape)
    return embeddings_2d, numerosities, obj_types


def get_unprojected_visual_embeddings(img_tensor, vision_tower, device):
    img_tensor = img_tensor.to(device)
    with torch.no_grad():
        vision_output = vision_tower(img_tensor)
        unproj_embeddings = vision_output.last_hidden_state
    return np.array(unproj_embeddings.squeeze(0).cpu())


def process_visual_unproj_embeddings(
    img_embeddings,
    max_count,
    dim_red_method='svd',
    pre_reduction=False,
    SEED=42,
    pre_reduction_dim=400,
    normalize=True
):
    img_embeddings_numpy = img_embeddings.cpu().numpy() if hasattr(img_embeddings, 'cpu') else np.array(img_embeddings)
    max_count = min(max_count, len(img_embeddings_numpy))
    img_embeddings_numpy = img_embeddings_numpy[:max_count]
    img_embeddings_numpy = img_embeddings_numpy.reshape(len(img_embeddings_numpy), -1)

    if normalize:
        img_embeddings_numpy = img_embeddings_numpy / np.linalg.norm(img_embeddings_numpy, axis=-1, keepdims=True)

    if pre_reduction:
        svd = TruncatedSVD(n_components=pre_reduction_dim, random_state=SEED)
        img_embeddings_numpy = svd.fit_transform(img_embeddings_numpy)

    img_embeddings_reduced = reduce_dimension(img_embeddings_numpy, 2, dim_red_method, SEED)
    return img_embeddings_reduced
    
def batch_collect_embeddings_with_cache(
    model_list,
    dataloader_list,    
    device='cuda',
    save_dir=None,
    dim_red_method='umap',
    normalize=True,
    pre_reduction=True,
    pre_reduction_dim=300,
    SEED=42,
    mode='cls'
):
    embeddings_dict = {}

    if save_dir is not None:
        os.makedirs(save_dir, exist_ok=True)

    for wrapped_model, trained_on in model_list:
        model = wrapped_model.vit.vit_peft.base_model.model
        for dataloader, dataset_name in dataloader_list:
            filename = f"{trained_on}__{dataset_name}.pt"
            filepath = os.path.join(save_dir, filename) if save_dir else None

            try:
                if filepath and os.path.exists(filepath):
                    print(f"Loading cached embeddings: {trained_on} / {dataset_name}")
                    data = torch.load(filepath,weights_only=False)
                    embeddings_dict[(trained_on, dataset_name)] = data
                else:
                    raise FileNotFoundError("Cache not found")
            except Exception:
                print(f"Cache loading failed or not found, collecting embeddings: {trained_on} / {dataset_name}")
                with torch.no_grad():
                    embeddings_2d, numerosities, obj_types = collect_unprojected_embeddings(
                        dataloader=dataloader,
                        vision_tower=model,
                        device=device,
                        dim_red_method=dim_red_method,
                        normalize=normalize,
                        pre_reduction=pre_reduction,
                        pre_reduction_dim=pre_reduction_dim,
                        SEED=SEED,
                        mode=mode
                    )

                embeddings_dict[(trained_on, dataset_name)] = (embeddings_2d, numerosities, obj_types)

                if filepath:
                    torch.save((embeddings_2d, numerosities, obj_types), filepath)
                    print(f"Saved embeddings to {filepath}")

    return embeddings_dict
    
    
    
def collect_unprojected_embeddings(
    dataloader,
    vision_tower,
    device='cuda',
    dim_red_method='svd',
    normalize=True,
    pre_reduction=False,
    pre_reduction_dim=400,
    SEED=42,
    mode='cls'
):
    import tqdm
    all_embeddings, numerosities, obj_types = [], [], []

    for batch in tqdm.tqdm(dataloader, desc="Extracting unproj embeddings"):
        img_tensors, labels, cat_names, cat_labels = batch
        img_tensors = img_tensors.to(device)

        unproj_embeds = get_unprojected_visual_embeddings(img_tensors, vision_tower, device)

        if mode == 'cls':
            unproj_embeds = unproj_embeds[:, 0, :]
        elif mode == 'patches':
            unproj_embeds = unproj_embeds[:, 1:, :].mean(dim=1)
        else:
            raise ValueError(f"Unknown mode: {mode}. Choose 'cls' or 'patches'.")



        all_embeddings.append(unproj_embeds)
        numerosities.extend([label.item() + 1 for label in labels])
        obj_types.extend(cat_names)

    all_embeddings = np.vstack(all_embeddings)
    numerosities = np.array(numerosities)
    obj_types = np.array(obj_types)

    print("Collected embeddings:", all_embeddings.shape)

    embeddings_2d = process_visual_unproj_embeddings(
        img_embeddings=all_embeddings,
        max_count=len(obj_types),
        dim_red_method=dim_red_method,
        pre_reduction=pre_reduction,
        SEED=SEED,
        pre_reduction_dim=pre_reduction_dim,
        normalize=normalize
    )

    print("Final reduced embedding shape:", embeddings_2d.shape)
    return embeddings_2d, numerosities, obj_types


# Helper function for dimensionality reduction
def reduce_dimension(data, n_components, method='svd', SEED=42):
    if method == 'svd':
        reducer = TruncatedSVD(n_components=n_components, random_state=SEED)
    elif method == 'pca':
        reducer = PCA(n_components=n_components, random_state=SEED)
    elif method == 'tsne':
        reducer = TSNE(n_components=n_components, random_state=SEED)
    elif method == 'umap':
        reducer = umap.UMAP(n_components=n_components, random_state=SEED)
    else:
        raise ValueError(f"Unknown reduction method: {method}")
    return reducer.fit_transform(data)

    
    

### --- ### --- ### --- ### --- ### --- ###### --- ### --- ### --- ### --- ### --- ###
##                              SAVING AND LOADING TOOLS
### --- ### --- ### --- ### --- ### --- ###### --- ### --- ### --- ### --- ### --- ###



def save_embeddings(save_path, embeddings, numerosities, obj_types, prefix):
    os.makedirs(save_path, exist_ok=True)
    np.save(os.path.join(save_path, f'embeddings_2d_{prefix}.npy'), embeddings)
    np.save(os.path.join(save_path, f'numerosities_{prefix}.npy'), numerosities)
    with open(os.path.join(save_path, f'obj_types_{prefix}.pkl'), 'wb') as f:
        pickle.dump(obj_types, f)
        
        
def load_embeddings(load_path, prefix):
    embeddings = np.load(os.path.join(load_path, f'embeddings_2d_{prefix}.npy'))
    numerosities = np.load(os.path.join(load_path, f'numerosities_{prefix}.npy'))
    with open(os.path.join(load_path, f'obj_types_{prefix}.pkl'), 'rb') as f:
        obj_types = pickle.load(f)
    return embeddings, numerosities, obj_types
    
    
    
    
def save_peft_vit_classifier(peft_classifier, save_path):
    os.makedirs(save_path, exist_ok=True)

    peft_model = peft_classifier.vit.vit_peft
    peft_model.save_pretrained(save_path)
    print(f"PEFT adapter saved to {save_path}")

    classifier_head_path = os.path.join(save_path, "classifier_head.pt")
    torch.save({
        'classifier_state_dict': peft_classifier.classifier.state_dict(),
        'num_classes': peft_classifier.classifier.out_features,
    }, classifier_head_path)
    print(f"Classifier head saved to {classifier_head_path}")


def load_peft_vit_classifier(base_vit, save_path, freeze_vit=False, device='cuda', load_class_head=True, merge=False):
    base_vit_fresh = deepcopy(base_vit)
    peft_model_loaded = PeftModel.from_pretrained(base_vit_fresh, save_path)

    if merge:
        try:
            print("Merging LoRA weights into base model...")
            peft_model_loaded = peft_model_loaded.merge_and_unload()
        except Exception as e:
            print(f"‚ö†Ô∏è Merge failed: {e}. Likely due to quantized layers. Continuing with adapters.")

    peft_wrapper = PeftViTWrapper(peft_model_loaded)

    classifier_head_path = os.path.join(save_path, "classifier_head.pt")
    checkpoint = torch.load(classifier_head_path, map_location=device)
    num_classes = checkpoint['num_classes']
    classifier = ViTClassifier(peft_wrapper, num_classes=num_classes, freeze_vit=freeze_vit).to(device)

    if load_class_head:
        classifier.classifier.load_state_dict(checkpoint['classifier_state_dict'])
        print(f"Loaded PEFT model and classifier head from {save_path}")
    else:
        print(f"Loaded PEFT model with randomly initialized classifier head")

    return classifier, peft_model_loaded







    
    

def batch_collect_embeddings(model_list, dataloader_list, device='cuda', save_dir=None,
                             dim_red_method='umap', normalize=True, pre_reduction=True,
                             pre_reduction_dim=300, SEED=42, mode='cls'):
    embeddings_dict = {}
    os.makedirs(save_dir, exist_ok=True) if save_dir else None

    for wrapped_model, trained_on in model_list:
        model = wrapped_model.vit.vit_peft.base_model.model
        for dataloader, dataset_name in dataloader_list:
            filepath = os.path.join(save_dir, f"{trained_on}__{dataset_name}.pt") if save_dir else None

            try:
                if filepath and os.path.exists(filepath):
                    print(f"Loading cached embeddings: {trained_on} / {dataset_name}")
                    embeddings_dict[(trained_on, dataset_name)] = torch.load(filepath, weights_only=False)
                else:
                    raise FileNotFoundError
            except Exception:
                print(f"Collecting embeddings: {trained_on} / {dataset_name}")
                embeddings = collect_embeddings(dataloader, model, device, dim_red_method,
                                                normalize, pre_reduction, pre_reduction_dim, SEED, mode)
                embeddings_dict[(trained_on, dataset_name)] = embeddings
                if filepath:
                    torch.save(embeddings, filepath)
                    print(f"Saved embeddings to {filepath}")
    return embeddings_dict
    
    
def collect_embeddings(
    dataloader, model, device, dim_red_method='pca', normalize=True,
    pre_reduction=False, pre_reduction_dim=400, SEED=42, mode='cls'
):
    import math

    all_embeds, numerosities, obj_types = [], [], []

    model.eval()
    with torch.no_grad():
        for imgs, labels, cat_names, _ in tqdm.tqdm(dataloader, desc="Extracting embeddings"):
            imgs = imgs.to(device)

            embeds = model(imgs).last_hidden_state
            if mode == 'cls':
                embeds = embeds[:, 0, :]
            else:
                embeds = embeds[:, 1:, :].mean(1)

            all_embeds.append(embeds.detach().cpu().numpy())
            numerosities.extend((labels + 1).numpy())
            obj_types.extend(cat_names)

    all_embeds = np.vstack(all_embeds)

    print("\n=== Raw Embeddings Shape ===")
    print(all_embeds.shape)

    # ----------------------------------
    # Normalization
    # ----------------------------------
    if normalize:
        all_embeds = sk_normalize(all_embeds)
        print("Embeddings normalized.")

    # ----------------------------------
    # Optional SVD pre-reduction
    # ----------------------------------
    if pre_reduction:
        print(f"Applying TruncatedSVD pre-reduction ‚Üí {pre_reduction_dim} dims")
        svd = TruncatedSVD(n_components=pre_reduction_dim, random_state=SEED)
        all_embeds = svd.fit_transform(all_embeds)
        print("After pre-reduction:", all_embeds.shape)

    # ----------------------------------
    # PCA main block
    # ----------------------------------
    if dim_red_method == 'pca':
        print("\n=== PCA Dimensionality Reduction ===")

        # Reduce to 2D for embedding
        pca_2 = PCA(n_components=2, random_state=SEED)
        embeds_2d = pca_2.fit_transform(all_embeds)

        print("\nExplained variance ratio (first 2 PCs):")
        print(pca_2.explained_variance_ratio_[:2])

        # Full PCA for scree
        max_components = min(50, all_embeds.shape[1])
        print(f"\nRunning full PCA for scree plot ({max_components} components)...")
        pca_full = PCA(n_components=max_components, random_state=SEED).fit(all_embeds)

        ev = pca_full.explained_variance_ratio_
        cumulative = np.cumsum(ev)

        print("\nExplained variance (first 20 components):")
        print(ev[:20])
        print("\nCumulative explained variance (first 20 components):")
        print(cumulative[:20])

        # Correct log-space indices
        indices = np.logspace(
            np.log10(1),
            np.log10(max_components),
            num=20
        )
        indices = np.ceil(indices).astype(int)
        indices = np.unique(indices)
        indices = indices[indices <= max_components]

        # Correct cumulative values
        cumulative_at_indices = cumulative[indices - 1]

        print("\nLog-spaced component counts:")
        print(indices)

        print("\nCumulative explained variance at those components:")
        for i, c in zip(indices, cumulative_at_indices):
            print(f"{i} components ‚Üí {c:.5f}")

        # Plot
        plt.figure(figsize=(6, 4))
        plt.plot(indices, cumulative_at_indices, marker='o')
        plt.xlabel("Number of components")
        plt.ylabel("Cumulative explained variance")
        plt.title("PCA Scree Plot")
        plt.grid(True)
        plt.show()

    elif dim_red_method == 'svd':
        embeds_2d = TruncatedSVD(n_components=2, random_state=SEED).fit_transform(all_embeds)

    elif dim_red_method == 'tsne':
        
        embeds_2d = TSNE(n_components=2, random_state=SEED).fit_transform(all_embeds)

    elif dim_red_method == 'umap':
        embeds_2d = umap.UMAP(n_components=2, random_state=SEED).fit_transform(all_embeds)

    else:
        raise ValueError(f"Unknown reduction method: {dim_red_method}")

    print("\nFinal reduced embedding shape:", embeds_2d.shape)
    return embeds_2d, np.array(numerosities), np.array(obj_types)





@dataclass
class ViTTrainingConfig:
    learning_rate: float
    weight_decay: float
    total_epochs: int
    batch_size_train: int
    batch_size_val: int
    mixed_precision: bool
    warmup_steps: int
    patience: int
    freeze_vit: bool
    
    
@dataclass
class ViTDatasetConfig:
    dataset_name: str
    train_split: float
    categories_train: List[str]
    categories_val: List[str]
    categories_test: List[str]
    nums: List[int]
    aug_factor_train: int
    undersample_ratio: float




@dataclass
class ViTLoRAConfig:
    r: int
    lora_alpha: float
    target_modules: List[str]


def load_config_vit(config_path: str):
    import json

    with open(config_path, "r") as f:
        cfg = json.load(f)

    vit_lora_config = ViTLoRAConfig(
        r=cfg["peft"]["r"],
        lora_alpha=cfg["peft"]["lora_alpha"],
        target_modules=cfg["peft"]["target_modules"],
    )

    vit_training_config = ViTTrainingConfig(
        learning_rate=cfg["training"]["learning_rate"],
        weight_decay=cfg["training"]["weight_decay"],
        total_epochs=cfg["training"]["total_epochs"],
        batch_size_train=cfg["training"]["batch_size_train"],
        batch_size_val=cfg["training"]["batch_size_val"],
        mixed_precision=cfg["training"]["mixed_precision"],
        warmup_steps=cfg["training"]["warmup_steps"],
        patience=cfg["training"]["patience"],
        freeze_vit=cfg["classifier"]["freeze_vit"],
    )

    vit_dataset_config = ViTDatasetConfig(
        dataset_name=cfg["dataset"]["train_name"],       # updated key
        train_split=cfg["dataset"]["train_name"],       # assuming train_name is used as split
        categories_train=cfg["dataset"]["train_objs"],  # updated key
        categories_val=cfg["dataset"]["val_objs"],      # updated key
        categories_test=cfg["dataset"]["test_objs"],    # updated key
        nums=None,                                      # your JSON has no "nums", can keep None
        aug_factor_train=cfg["dataset"]["augment_factor"], # updated key
        undersample_ratio=None                          # your JSON has no undersample_ratio
    )

    vit_experiment_config = ViTExperimentConfig(
        save_path=cfg["paths"]["results_save"],         # updated key
        num_classes=cfg["classifier"]["num_classes"],
    )

    print("=" * 60)
    print("VIT CONFIGURATION SUMMARY")
    print("=" * 60)

    print("\nüìä DATASET")
    print(f"  Dataset: {vit_dataset_config.dataset_name}")
    print(f"  Train split: {vit_dataset_config.train_split}")
    print(f"  Train categories: {vit_dataset_config.categories_train}")
    print(f"  Val categories: {vit_dataset_config.categories_val}")
    print(f"  Test categories: {vit_dataset_config.categories_test}")
    print(f"  Numbers: {vit_dataset_config.nums}")
    print(f"  Aug factor (train): {vit_dataset_config.aug_factor_train}")

    print("\nüéØ TRAINING")
    print(f"  LR: {vit_training_config.learning_rate}")
    print(f"  Weight decay: {vit_training_config.weight_decay}")
    print(f"  Epochs: {vit_training_config.total_epochs}")
    print(f"  Batch size (train): {vit_training_config.batch_size_train}")
    print(f"  Batch size (val): {vit_training_config.batch_size_val}")
    print(f"  Mixed precision: {vit_training_config.mixed_precision}")
    print(f"  Warmup steps: {vit_training_config.warmup_steps}")
    print(f"  Patience: {vit_training_config.patience}")
    print(f"  Freeze ViT: {vit_training_config.freeze_vit}")

    print("\nüîß PEFT (LoRA)")
    print(f"  r: {vit_lora_config.r}")
    print(f"  alpha: {vit_lora_config.lora_alpha}")
    print(f"  target modules: {', '.join(vit_lora_config.target_modules)}")

    print("\nüíæ EXPERIMENT")
    print(f"  Save path: {vit_experiment_config.save_path}")
    print(f"  Num classes: {vit_experiment_config.num_classes}")

    print("=" * 60)

    return (
        vit_dataset_config,
        vit_training_config,
        vit_lora_config,
        vit_experiment_config,
    )


def train_vit_regressor(
    model,
    train_loader,
    val_loader,
    device=None,
    on_cats=False,
    num_epochs=10,
    lr=1e-4,
    weight_decay=0.01,
    mixed_prec_train=False,
    warmup_steps=500,
    patience=3
):

    model.to(device)
    model.train()

    # Detect if model outputs multiple classes or single value
    sample_batch = next(iter(train_loader))
    if on_cats:
        sample_images, _, _, _ = sample_batch
    else:
        sample_images, _, _, _ = sample_batch

    with torch.no_grad():
        sample_output = model(sample_images[:1].to(device))
        is_classification_head = sample_output.shape[-1] > 1

    print(f"Model output shape: {sample_output.shape}")
    print(f"Detected {'classification' if is_classification_head else 'regression'} head")

    if is_classification_head:
        print("‚ö†Ô∏è  Using classification head for regression task")
        print("   Converting logits to class predictions then treating as continuous")
        criterion = nn.CrossEntropyLoss()  # Use CE loss for classification head
    else:
        print("‚úì Using proper regression head")
        criterion = RMSELoss()

    # Optimizer
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=lr,
        weight_decay=weight_decay
    )

    total_steps = num_epochs * len(train_loader)

    def lr_lambda(current_step):
        if current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return 0.5 * (1.0 + math.cos(math.pi * progress))

    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    scaler = torch.amp.GradScaler('cuda', enabled=mixed_prec_train)

    train_losses, train_accs, val_losses, val_accs = [], [], [], []

    for epoch in range(1, num_epochs + 1):
        # --- Training ---
        model.train()
        train_loss, correct_train, total_train = 0.0, 0, 0
        loop = tqdm.tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs} [Train]", leave=False)

        for batch in loop:
            if on_cats:
                images, _, _, labels = batch
            else:
                images, labels, _, _ = batch

            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            with torch.amp.autocast('cuda', enabled=mixed_prec_train):
                logits = model(images)

                if is_classification_head:
                    # Classification head: use CrossEntropy loss
                    # Labels are class indices (0-9)
                    loss = criterion(logits, labels.long())
                else:
                    # Regression head: use MSE loss
                    preds = logits.squeeze()
                    loss = criterion(preds, labels.float())

            # Backprop
            if mixed_prec_train:
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()

            scheduler.step()

            # Metrics
            train_loss += loss.item() * images.size(0)

            with torch.no_grad():
                if is_classification_head:
                    pred_class = torch.argmax(logits, dim=1)
                else:
                    pred_class = torch.round(logits.squeeze()).clamp(min=0, max=9).long()

                correct_train += (pred_class == labels).sum().item()

            total_train += labels.size(0)
            loop.set_postfix(loss=train_loss / total_train, acc=correct_train / total_train)

        train_loss /= total_train
        train_acc = correct_train / total_train

        # --- Validation ---
        model.eval()
        val_loss, correct_val, total_val = 0.0, 0, 0
        loop_val = tqdm.tqdm(val_loader, desc=f"Epoch {epoch}/{num_epochs} [Val]", leave=False)

        with torch.no_grad(), torch.amp.autocast('cuda', enabled=mixed_prec_train):
            for batch in loop_val:
                if on_cats:
                    images, _, _, labels = batch
                else:
                    images, labels, _, _ = batch

                images, labels = images.to(device), labels.to(device)
                logits = model(images)

                if is_classification_head:
                    loss = criterion(logits, labels.long())
                    pred_class = torch.argmax(logits, dim=1)
                else:
                    preds = logits.squeeze()
                    loss = criterion(preds, labels.float())
                    pred_class = torch.round(preds).clamp(min=0, max=9).long()

                val_loss += loss.item() * images.size(0)
                correct_val += (pred_class == labels).sum().item()
                total_val += labels.size(0)

                loop_val.set_postfix(loss=val_loss / total_val, acc=correct_val / total_val)

        val_loss /= total_val
        val_acc = correct_val / total_val

        train_losses.append(train_loss)
        train_accs.append(train_acc)
        val_losses.append(val_loss)
        val_accs.append(val_acc)

        print(f"Epoch {epoch}/{num_epochs} => "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    return train_losses, train_accs, val_losses, val_accs


def convert_classifier_to_regressor(model, device='cuda'):

    # Find the classifier layer
    if hasattr(model, 'classifier'):
        in_features = model.classifier.in_features
        model.classifier = nn.Linear(in_features, 1).to(device)
        print(f"‚úì Replaced classifier: {in_features} -> 1 (regression)")
    elif hasattr(model, 'head'):
        in_features = model.head.in_features
        model.head = nn.Linear(in_features, 1).to(device)
        print(f"‚úì Replaced head: {in_features} -> 1 (regression)")
    else:
        print("‚ö†Ô∏è  Could not find classifier layer to replace")
        print("   Available attributes:", dir(model))

    return model
    
    

class RMSELoss(nn.Module):
    def __init__(self, eps=1e-8):
        super().__init__()
        self.mse = nn.MSELoss()
        self.eps = eps

    def forward(self, pred, target):
        return torch.sqrt(self.mse(pred, target) + self.eps)

    
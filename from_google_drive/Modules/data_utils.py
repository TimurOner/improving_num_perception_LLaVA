# -*- coding: utf-8 -*-
"""data_utils

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IlBxxcm84XpgySuagHx4eSG3G8DTkzDk
"""

import os
import random
import itertools
import json


from dataclasses import dataclass
from typing import List, Optional


import torch
from torch.utils.data import Dataset, DataLoader,Subset
import torchvision
from torchvision import transforms

from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import cv2

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

from transformers import AutoProcessor

model_name = "llava-hf/llava-1.5-7b-hf"
processor = AutoProcessor.from_pretrained(model_name)
processor.patch_size = 14
processor.vision_feature_select_strategy = "default"
NUMS = list(range(1, 11))
BASE_PATH = "/content/drive/MyDrive/Modality Gap and Numerosity Thesis"
BATCH_SIZES = {"train": 8, "val": 32, "test": 32}  # adjust if needed
DATASET_PATHS = {
    "CCNL": f"{BASE_PATH}/models/images",
    "CCNL2": f"{BASE_PATH}/CCNL2",
    "CLEVR_NEW": f"{BASE_PATH}/CLEVR_new",
    "CLEVR":f"{BASE_PATH}/CLEVR",
    "CHLDBOOK": f"{BASE_PATH}/childbook"
}

OBJ_SPLITS = {
    "CCNL": {
        "train": ['apples', 'squares', 'people'],
        "val": ['circles','dots'],
        "test": ['butterflies', 'triangles']
    },
    "CCNL2": {
        "train": ['apples', 'cups'],
        "val": ['buttons'],
        "test": ['bottles', 'knives']
    }
}

class ImageDatasetVit(Dataset):
    def __init__(self, image_base_path, dataset, processor, nums=None, categories=None, transform=None, undersamp_ratio=1.0, aug_factor=1):
        self.image_paths, self.labels, self.cats = [], [], []
        self.transform, self.processor, self.dataset = transform, processor, dataset
        all_img_files = os.listdir(image_base_path)
        for img_file in all_img_files:
            true_num = self._get_true_num(img_file)
            cat_name = self._get_category(img_file)
            if (nums is None or true_num in nums) and (categories is None or cat_name in categories):
                self.image_paths.append(os.path.join(image_base_path, img_file))
                self.labels.append(true_num)
                self.cats.append(cat_name)
        if 0 < undersamp_ratio < 1.0:
            total_samples = len(self.image_paths)
            keep_count = int(total_samples * undersamp_ratio)
            indices = random.sample(range(total_samples), keep_count)
            self.image_paths = [self.image_paths[i] for i in indices]
            self.labels = [self.labels[i] for i in indices]
            self.cats = [self.cats[i] for i in indices]
        unique_cats = sorted(list(set(self.cats)))
        self.cat_to_idx = {cat: idx for idx, cat in enumerate(unique_cats)}
        self.idx_to_cat = {idx: cat for cat, idx in self.cat_to_idx.items()}
        self.aug_factor = max(1, int(aug_factor))
        if self.aug_factor > 1:
            self.image_paths *= self.aug_factor
            self.labels *= self.aug_factor
            self.cats *= self.aug_factor

    def _get_true_num(self, img_name):
        parts = img_name.split('_')
        if self.dataset == 'CLEVR':
            return int(parts[0])
        for part in parts:
            if part.isdigit() and int(part) < 100:
                return int(part)
        return None

    def _get_category(self, img_name):
        parts = img_name.split('_')
        if parts and (self.dataset == 'CCNL' or self.dataset == 'CCNL2'):
            return parts[1] if len(parts) > 1 else ""
        elif parts and self.dataset == 'CLEVR':
            return parts[0] if parts else ""
        else:
            return ""

    def __len__(self):
        return len(self.image_paths)
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image_PIL = Image.open(image_path).convert("RGB")
        if self.transform is not None and (self.aug_factor > 1):
            original_idx = idx % (len(self.image_paths) // self.aug_factor)
            if idx >= len(self.image_paths) // self.aug_factor:
                image_PIL = self.transform(image_PIL)
        image = torch.tensor(self.processor.image_processor(image_PIL)['pixel_values'][0])
        label = self.labels[idx] - 1
        cat_name = self.cats[idx]
        cat_label = self.cat_to_idx[cat_name]
        return image, label, cat_name, cat_label
    def get_image_PIL(self,idx):
        image_path = self.image_paths[idx]
        image_PIL = Image.open(image_path).convert("RGB")
        return image_PIL

class ImageTextDataset(Dataset):
    def __init__(
        self,
        image_base_path,
        dataset,
        processor,
        nums,
        categories,
        test_mode=False,
        undersamp_ratio=1.0,
        sample_idx_range=None  # <-- new argument
    ):
        self.base_path = image_base_path
        self.processor = processor
        self.dataset = dataset
        self.test_mode = test_mode
        self.sample_idx_range = sample_idx_range
        self.undersamp_ratio = undersamp_ratio

        self.base_prompt = (
            "USER: <image>\nHow many objects are there in the image? "
            "Respond only with the number.\nASSISTANT:"
        )

        self.image_paths = []
        self.labels_int = []
        self.obj_categories = []

        all_img_files = os.listdir(image_base_path)
        img_path_list = []
        labels_list = []

        category_counts = {}
        numerosity_counts = {}

        for img_file in all_img_files:
            true_num = self.__get_true_num(img_file)
            cat_name = self.__get_category(img_file)

            # Category filter
            if cat_name in categories:
                self.obj_categories.append(cat_name)

            # Usual dataset filtering
            keep = False
            if self.dataset == 'CLEVR':
                keep = true_num is not None and true_num in nums
            elif self.dataset in ['CHLDBOOK']:
                keep = true_num is not None and true_num in nums
            elif self.dataset in ['CCNL', 'CCNL2']:
                keep = (
                    true_num is not None
                    and true_num in nums
                    and cat_name in categories
                )

            if not keep:
                continue

            # Sample index filtering for CCNL2
            if self.dataset == 'CCNL2' and self.sample_idx_range is not None:
                sample_idx = self.__get_sample_idx(img_file)
                if sample_idx not in self.sample_idx_range:
                    continue

            full_path = os.path.join(image_base_path, img_file)
            img_path_list.append(full_path)
            labels_list.append(true_num)
            category_counts[cat_name] = category_counts.get(cat_name, 0) + 1
            numerosity_counts[true_num] = numerosity_counts.get(true_num, 0) + 1

        # Optional undersampling
        if undersamp_ratio < 1.0 and img_path_list:
            combined = list(zip(img_path_list, labels_list))
            random.shuffle(combined)
            keep_len = max(1, int(len(combined) * undersamp_ratio))
            combined = combined[:keep_len]
            img_path_list, labels_list = zip(*combined)
            img_path_list, labels_list = list(img_path_list), list(labels_list)

        self.image_paths = img_path_list
        self.labels_int = labels_list

    def __len__(self):
        return len(self.image_paths)

    def __get_true_num(self, img_name):
        parts = img_name.split('_')
        if self.dataset == 'CHLDBOOK':
            return int(parts[1])
        elif self.dataset in ['CCNL', 'CCNL2', 'CLEVR']:
            for part in parts:
                if part.isdigit() and int(part) < 11:
                    return int(part)
        return None

    def __get_category(self, img_name):
        parts = img_name.split('_')
        if parts and self.dataset in ['CCNL', 'CCNL2']:
            return parts[1] if len(parts) > 1 else ""
        return None

    def __get_sample_idx(self, img_name):
        if self.dataset == 'CCNL2':
            parts = img_name.split('_')
            return int(parts[-1].split('.')[0]) % 50
        return None

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert("RGB")
        original_numerical_label = self.labels_int[idx]

        full_text_training = f"{self.base_prompt} {original_numerical_label}"
        full_text_inference = f"{self.base_prompt} "

        if not self.test_mode:
            processed = self.processor(
                images=image, text=full_text_training, return_tensors="pt"
            )
            input_ids = processed.input_ids.squeeze(0)
            attention_mask = processed.attention_mask.squeeze(0)
            pixel_values = processed.pixel_values.squeeze(0)

            prompt_only = self.processor(
                images=image, text=full_text_inference, return_tensors="pt"
            )
            mask_len = prompt_only.input_ids.squeeze(0).shape[0]

            labels = input_ids.clone()
            labels[:mask_len] = -100
            labels[labels == self.processor.tokenizer.pad_token_id] = -100

        else:
            processed = self.processor(
                images=image, text=full_text_inference, return_tensors="pt"
            )
            input_ids = processed.input_ids.squeeze(0)
            attention_mask = processed.attention_mask.squeeze(0)
            pixel_values = processed.pixel_values.squeeze(0)
            labels = -100 * torch.ones_like(input_ids)

        return (
            pixel_values,
            input_ids,
            attention_mask,
            labels,
            original_numerical_label,
            image_path,
        )

    def get_item_metadata(self, idx):
        return self.labels_int[idx], self.image_paths[idx]

    @property
    def categories(self):
        return self.obj_categories

        
class YesNoDataset(Dataset):
    def __init__(self, dataset_instance, nums_per_image=2,
                 use_basic_augmentation=False, only_diff=False, test_mode=False,
                 diff_amount=None):
        self.dataset = dataset_instance
        self.nums_per_image = nums_per_image
        self.use_basic_augmentation = use_basic_augmentation
        self.only_diff = only_diff
        self.test_mode = test_mode
        self.diff_amount = diff_amount

        if self.use_basic_augmentation and not self.test_mode:
            self.augmentation = transforms.Compose([
                transforms.ColorJitter(brightness=0.2, contrast=0.2,
                                       saturation=0.2, hue=0.1),
                transforms.RandomRotation(degrees=15),
                transforms.RandomHorizontalFlip(p=0.5),
            ])
        else:
            self.augmentation = None

        self.samples_per_image = 1 + self.nums_per_image * 2
        if self.only_diff:
            self.effective_len = len(self.dataset) * (self.samples_per_image - 1)
        else:
            self.effective_len = len(self.dataset) * self.samples_per_image

    def __len__(self):
        return self.effective_len

    @property
    def obj_categories(self):
        categories = []
        for image_path in self.dataset.image_paths:
            img_name = os.path.basename(image_path)
            parts = img_name.split('_')
            if parts and self.dataset.dataset in ['CCNL', 'CCNL2']:
                cat = parts[1] if len(parts) > 1 else ""
            else:
                cat = ""
            variations_count = self.samples_per_image if not self.only_diff else (self.samples_per_image - 1)
            categories.extend([cat] * variations_count)
        return categories

    def __getitem__(self, idx):
        if self.only_diff:
            image_idx = idx // (self.samples_per_image - 1)
            variation_idx = (idx % (self.samples_per_image - 1)) + 1
        else:
            image_idx = idx // self.samples_per_image
            variation_idx = idx % self.samples_per_image

        original_label, image_path = self.dataset.get_item_metadata(image_idx)
        image = Image.open(image_path).convert("RGB")

        if self.augmentation is not None:
            image = self.augmentation(image)

        rng = random.Random(idx)

        if self.diff_amount is not None:
            should_be_yes = rng.choice([True, False])

            if variation_idx % 2 == 1:
                if should_be_yes:
                    D = max(0, original_label - self.diff_amount)
                else:
                    D = original_label + self.diff_amount
            else:
                if should_be_yes:
                    D = original_label + self.diff_amount
                else:
                    D = max(0, original_label - self.diff_amount)
        else:
            D = rng.randint(1, 10)

        if not self.only_diff and variation_idx == 0:
            prompt_text = f"USER: <image>\nAre there exactly {original_label} objects in this image? Please answer with yes or no.\nASSISTANT:"
            label = "yes"
            if not self.test_mode:
                prompt_text = f"{prompt_text} Yes"
        else:
            if variation_idx % 2 == 1:
                label = 'yes' if original_label > D else 'no'
                prompt_text = f"USER: <image>\nAre there more than {D} objects in this image? Please answer with yes or no.\nASSISTANT:"
            else:
                label = 'yes' if original_label < D else 'no'
                prompt_text = f"USER: <image>\nAre there less than {D} objects in this image? Please answer with yes or no.\nASSISTANT:"

            if not self.test_mode:
                prompt_text = f"{prompt_text} {'Yes' if label == 'yes' else 'No'}"

        processed = self.dataset.processor(
            images=image,
            text=prompt_text,
            return_tensors="pt"
        )

        input_ids = processed.input_ids.squeeze(0)
        attention_mask = processed.attention_mask.squeeze(0)
        pixel_values = processed.pixel_values.squeeze(0)

        labels = input_ids.clone()

        if self.test_mode:
            labels[:] = -100
        else:
            prompt_only_text = prompt_text.rsplit(" ", 1)[0]
            prompt_only_processed = self.dataset.processor(
                images=image,
                text=prompt_only_text,
                return_tensors="pt"
            )
            mask_len = prompt_only_processed.input_ids.squeeze(0).shape[0]
            labels[:mask_len] = -100
            labels[labels == self.dataset.processor.tokenizer.pad_token_id] = -100

        return pixel_values, input_ids, attention_mask, labels, label, image_path
       
class ExpandedDifferenceDataset(Dataset):
    def __init__(self, dataset_instance, diff_nums_per_image=2, use_basic_augmentation=False):
        self.dataset = dataset_instance
        self.diff_nums_per_image = diff_nums_per_image
        self.use_basic_augmentation = use_basic_augmentation

        # Define basic augmentation if enabled
        if self.use_basic_augmentation:
            self.augmentation = transforms.Compose([
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.RandomRotation(degrees=15),
                transforms.RandomHorizontalFlip(p=0.5),
            ])
        else:
            self.augmentation = None

        self.prompt_template_more = (
            "USER: <image>\n"
            "How many more objects are there in the image than %d?\n"
            "ASSISTANT:"
        )
        self.prompt_template_fewer = (
            "USER: <image>\n"
            "How many fewer objects are there in the image than %d?\n"
            "ASSISTANT:"
        )

        self.samples_per_image = 1 + self.diff_nums_per_image
        self.effective_len = len(self.dataset) * self.samples_per_image

        print(f"[ExpandedDifferenceDataset] Each image generates {self.samples_per_image} samples")
        print(f"Effective dataset length: {self.effective_len}")

    def __len__(self):
        return self.effective_len

    def __getitem__(self, idx):
        image_idx = idx // self.samples_per_image
        variation_idx = idx % self.samples_per_image

        original_label, image_path = self.dataset.get_item_metadata(image_idx)
        image = Image.open(image_path).convert("RGB")

        # Apply basic augmentation if enabled
        if self.use_basic_augmentation and not self.dataset.test_mode:
            image = self.augmentation(image)

        if variation_idx == 0:
            # Original count prompt
            prompt_text = f"{self.dataset.base_prompt} {original_label}"
            answer = original_label
        else:
            # Bidirectional difference prompt
            if random.random() < 0.5:
                compare_number = random.randint(1, original_label)
                prompt_text = self.prompt_template_more % compare_number
                answer = max(original_label - compare_number, 0)
            else:
                compare_number = random.randint(original_label, 11)
                prompt_text = self.prompt_template_fewer % compare_number
                answer = max(compare_number - original_label, 0)

            prompt_text = f"{prompt_text} {answer}"

        # Process image + text
        processed = self.dataset.processor(
            images=image,
            text=prompt_text,
            return_tensors="pt"
        )
        input_ids = processed.input_ids.squeeze(0)
        attention_mask = processed.attention_mask.squeeze(0)
        pixel_values = processed.pixel_values.squeeze(0)

        # Mask prompt tokens
        prompt_only_text = prompt_text.rsplit(" ", 1)[0]
        prompt_only_processed = self.dataset.processor(
            images=image,
            text=prompt_only_text,
            return_tensors="pt"
        )
        mask_len = prompt_only_processed.input_ids.squeeze(0).shape[0]

        labels = input_ids.clone()
        labels[:mask_len] = -100
        labels[labels == self.dataset.processor.tokenizer.pad_token_id] = -100

        # Return consistent tuple
        meta = {
            "image_path": image_path,
            "original_label": original_label,
            "variation_idx": variation_idx
        }
        if variation_idx != 0:
            meta["compare_number"] = compare_number
            meta["difference_answer"] = answer

        return pixel_values, input_ids, attention_mask, labels, answer, image_path
        
        
def visualize_vit_transform(dataset_untrans, dataset_trans, idx_list):

    mean = torch.tensor([0.485, 0.456, 0.406])
    std = torch.tensor([0.229, 0.224, 0.225])

    def denormalize(tensor):
        tensor = tensor.clone()
        for t, m, s in zip(tensor, mean, std):
            t.mul_(s).add_(m)
        return torch.clamp(tensor, 0, 1)

    def draw_grid(ax, H, W, grid_size=14, color="white", lw=0.5):
       
        step_x = W / grid_size
        step_y = H / grid_size

        # Vertical grid lines
        for i in range(1, grid_size):
            ax.axvline(i * step_x, color=color, linewidth=lw)

        # Horizontal grid lines
        for i in range(1, grid_size):
            ax.axhline(i * step_y, color=color, linewidth=lw)

    for idx in idx_list:
        img_untrans, _, _, _ = dataset_untrans[idx]
        img_trans, _, _, _ = dataset_trans[idx]

        orig_disp = denormalize(img_untrans).permute(1, 2, 0).numpy()
        trans_disp = denormalize(img_trans).permute(1, 2, 0).numpy()

        H, W = orig_disp.shape[:2]  # typically 224Ã—224

        plt.figure(figsize=(10, 5))

        # Original image
        ax1 = plt.subplot(1, 2, 1)
        ax1.imshow(orig_disp)
        draw_grid(ax1, H, W, grid_size=14)
        ax1.set_title(f"Original idx {idx}")
        ax1.axis("off")

        # Transformed image
        ax2 = plt.subplot(1, 2, 2)
        ax2.imshow(trans_disp)
        draw_grid(ax2, H, W, grid_size=14)
        ax2.set_title("Transformed (denormalized)")
        ax2.axis("off")

        plt.tight_layout()
        plt.show()

    
def create_per_category_datasets_direct(dataset_name, obj_splits):
   
    all_categories = obj_splits["train"] + obj_splits["val"] + obj_splits["test"]
    per_category_datasets = []

    for cat in all_categories:
        ds = make_dataset(dataset_name, num_scaling=False, categories=[cat])
        per_category_datasets.append(ds)

    return per_category_datasets, all_categories
    
    

####### Augmentation classes and functions ######

class TransformedSubset(Dataset):
    def __init__(self, subset, transform):
        self.subset = subset
        self.transform = transform

    def __len__(self):
        return len(self.subset)

    def __getitem__(self, idx):
        img, label, cat, cat_label = self.subset[idx]
        img = self.transform(img)
        return img, label, cat, cat_label


class AffineWithBackground:
    def __init__(self, degrees=10, translate=0.0, scale=(0.95, 1.05), shear=0.0):
        self.degrees = degrees
        self.translate = translate
        self.scale = scale
        self.shear = shear

    def __call__(self, img: Image.Image):
        angle = random.uniform(-self.degrees, self.degrees)
        if isinstance(self.scale, tuple):
            scale_factor = random.uniform(self.scale[0], self.scale[1])
        else:
            scale_factor = self.scale
        if isinstance(self.shear, tuple):
            shear_angle = random.uniform(self.shear[0], self.shear[1])
        else:
            shear_angle = random.uniform(-self.shear, self.shear)
        
        max_dx = self.translate * img.width
        max_dy = self.translate * img.height
        translations = (random.uniform(-max_dx, max_dx), random.uniform(-max_dy, max_dy))

        bg_color = img.getpixel((0, 0))

        return torchvision.transforms.functional.affine(
            img,
            angle=angle,
            translate=translations,
            scale=scale_factor,
            shear=shear_angle,
            fill=bg_color
        )
class AddPixelNoise:
    """Adds small Gaussian noise to a tensor image in [0,1] range."""
    def __init__(self, std=0.05):
        self.std = std

    def __call__(self, img):
        # If img is PIL.Image, convert to tensor first
        if not isinstance(img, torch.Tensor):
            img = transforms.ToTensor()(img)
        noise = torch.randn_like(img) * self.std
        img = img + noise
        return torch.clamp(img, 0, 1)  # ensure values stay in [0,1]

class GrayscaleObjectsTransform:
    def __init__(self, target_gray_range=(100, 150), method='uniform'):
        self.target_gray_range = target_gray_range
        self.method = method
    
    def __call__(self, image):
        if isinstance(image, torch.Tensor):
            img_np = image.permute(1, 2, 0).numpy()
            if img_np.max() <= 1.0:
                img_np = (img_np * 255).astype(np.uint8)
        elif isinstance(image, Image.Image):
            img_np = np.array(image)
        else:
            img_np = image
        
        if self.method == 'uniform':
            result = self._uniform_gray(img_np)
        elif self.method == 'slight_variation':
            result = self._slight_variation(img_np)
        elif self.method == 'preserve_luminance':
            result = self._preserve_luminance(img_np)
        
        if isinstance(image, torch.Tensor):
            return torch.from_numpy(result).permute(2, 0, 1).float() / 255.0
        elif isinstance(image, Image.Image):
            return Image.fromarray(result)
        return result
    
    def _uniform_gray(self, img_np):
        """Convert all objects to same gray tone"""
        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        
        # Detect background (gray, low saturation)
        lower_gray = np.array([0, 0, 50])
        upper_gray = np.array([180, 50, 200])
        background_mask = cv2.inRange(hsv, lower_gray, upper_gray)
        object_mask = cv2.bitwise_not(background_mask)
        
        # Create uniform gray for objects
        target_gray = np.random.randint(*self.target_gray_range)
        gray_img = np.full_like(img_np, target_gray)
        
        # Keep background, replace objects with gray
        result = np.where(object_mask[:, :, None] > 0, gray_img, img_np)
        
        return result
    
    def _slight_variation(self, img_np):
        
        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        
        lower_gray = np.array([0, 0, 50])
        upper_gray = np.array([180, 50, 200])
        background_mask = cv2.inRange(hsv, lower_gray, upper_gray)
        object_mask = cv2.bitwise_not(background_mask)
        
        # Find connected components (individual objects)
        num_labels, labels = cv2.connectedComponents(object_mask)
        
        result = img_np.copy()
        
        # Assign each object a slightly different gray
        for label_id in range(1, num_labels):
            obj_mask = (labels == label_id)
            if obj_mask.sum() < 100:  # Skip tiny noise
                continue
            
            # Random gray within narrow range
            gray_value = np.random.randint(
                self.target_gray_range[0], 
                self.target_gray_range[1]
            )
            result[obj_mask] = [gray_value, gray_value, gray_value]
        
        return result
    
    def _preserve_luminance(self, img_np):
        """Convert to grayscale but preserve original luminance"""
        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        
        lower_gray = np.array([0, 0, 50])
        upper_gray = np.array([180, 50, 200])
        background_mask = cv2.inRange(hsv, lower_gray, upper_gray)
        object_mask = cv2.bitwise_not(background_mask)
        
        # Convert to grayscale (removes color, keeps luminance)
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        gray_rgb = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
        
        # Apply only to objects, keep background
        result = np.where(object_mask[:, :, None] > 0, gray_rgb, img_np)
        
        return result

class RandomGrayscaleObjectsTransform:
    def __init__(self, target_gray_range=(100, 150), method='preserve_luminance', p=0.5):
        self.p = p
        self.transform = GrayscaleObjectsTransform(target_gray_range=target_gray_range, method=method)
    
    def __call__(self, img):
        if random.random() < self.p:
            return self.transform(img)
        return img
class AddGaussianNoise:
    def __init__(self, mean=0.0, std=0.05):
        self.mean = mean
        self.std = std

    def __call__(self, img):
        tensor = transforms.ToTensor()(img)
        noise = torch.randn(tensor.size()) * self.std + self.mean
        tensor = tensor + noise
        tensor = torch.clamp(tensor, 0.0, 1.0)
        return transforms.ToPILImage()(tensor)

def custom_collate_fn(batch):
    from torch.nn.utils.rnn import pad_sequence
    pixel_values = torch.stack([item[0] for item in batch])  # shape: (B, C, H, W)
    
    input_ids = [item[1] for item in batch]
    attention_masks = [item[2] for item in batch]
    labels = [item[3] for item in batch]
    
    # Assumes tokenizer.pad_token_id == 0, which is typical
    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_mask_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)
    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)

    numerical_labels = torch.tensor([item[4] for item in batch], dtype=torch.long)
    image_paths = [item[5] for item in batch]

    return (
       pixel_values,
       input_ids_padded,
       attention_mask_padded,
       labels_padded,
       numerical_labels,
       image_paths)
def make_dataset(dataset_name, num_scaling=False, scale_factors_custom=None, categories=None, aug_factor=1, undersamp_ratio=1, vit_transform=None):
        return ImageDatasetVit(
            image_base_path=DATASET_PATHS[dataset_name],
            dataset=dataset_name,
            processor=processor,
            nums=NUMS,
            categories=categories,
            transform=vit_transform,
            undersamp_ratio=undersamp_ratio,
            aug_factor=aug_factor
        )
        


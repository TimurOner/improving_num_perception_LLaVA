# -*- coding: utf-8 -*-
"""val_test_funs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BDKGEosqGR8nZLmTa0UNMhWzFIFYwqKZ
"""

import random
from copy import deepcopy
import warnings

import numpy as np


import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.patches as patches

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from scipy.spatial import ConvexHull


def plot_visual_embedding_scatter(
    visual_embeddings,
    numerosity_array,
    obj_type_array,
    title="Visual Embedding Scatter Plot",
    border=None,
    averages=False,
    plot_convex=False
):

    print(f"[INFO] Embeddings shape: {visual_embeddings.shape}")
    print(f"[INFO] Numerosity array length: {len(numerosity_array)}")
    print(f"[INFO] Object type array length: {len(obj_type_array)}")

    assert visual_embeddings.shape[0] == len(numerosity_array) == len(obj_type_array), \
        "[ERROR] Length mismatch between embeddings, numerosity_array, and obj_type_array."
    assert visual_embeddings.shape[1] == 2, \
        "[ERROR] visual_embeddings must be 2D (N x 2) for scatter plot and convex hull."

    cmap = cm.viridis_r
    num_cats = np.unique(numerosity_array)

    fig, ax = plt.subplots(figsize=(10, 8))

    for i, category in enumerate(num_cats):
        idxs = np.where(numerosity_array == category)
        color = cmap(i / len(num_cats))

        if averages:
            avg_x = np.mean(visual_embeddings[idxs, 0])
            avg_y = np.mean(visual_embeddings[idxs, 1])
            ax.scatter(
                avg_x,
                avg_y,
                label=f'Number {category} (avg)',
                s=100,
                color=color,
                marker='x',
                linewidths=3,
                alpha=0.9
            )
        else:
            ax.scatter(
                visual_embeddings[idxs, 0],
                visual_embeddings[idxs, 1],
                label=f'Number {category}',
                s=100,
                color=color,
                alpha=0.7
            )

    if plot_convex:
        obj_types = np.unique(obj_type_array)
        hull_colors = cm.Set3(np.linspace(0, 1, len(obj_types)))

        for obj_type, hull_color in zip(obj_types, hull_colors):
            obj_idxs = np.where(obj_type_array == obj_type)[0]
            points = visual_embeddings[obj_idxs]

            print(f"[DEBUG] Object type: {obj_type}, Points: {len(points)}")

            if len(points) < 3:
                print(f"[WARN] Skipping {obj_type}: Not enough points for a convex hull.")
                continue

            try:
                hull = ConvexHull(points)
                for simplex in hull.simplices:
                    ax.plot(points[simplex, 0], points[simplex, 1],
                            color=hull_color, alpha=0.6, linewidth=2)

                hull_points = points[hull.vertices]
                hull_patch = patches.Polygon(hull_points, alpha=0.1, facecolor=hull_color)
                ax.add_patch(hull_patch)

                centroid_x = np.mean(hull_points[:, 0])
                centroid_y = np.mean(hull_points[:, 1])
                ax.text(centroid_x, centroid_y, str(obj_type),
                        fontsize=10, fontweight='bold', ha='center', va='center')

            except Exception as e:
                print(f"[ERROR] Convex hull failed for object type {obj_type}: {e}")

    ax.set_xlabel('Embedding Dim 1')
    ax.set_ylabel('Embedding Dim 2')
    ax.set_title(title)
    ax.grid(True)
    ax.legend()

    if border is not None:
        if isinstance(border, (list, tuple)) and len(border) == 4:
            ax.set_xlim(border[0], border[1])
            ax.set_ylim(border[2], border[3])
        elif border is False:
            for spine in ax.spines.values():
                spine.set_visible(False)

    plt.tight_layout()
    plt.show()


def plot_category_wise_results(results_dict, title=None, save_path=None):
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

    # -------------------------------------------------------------
    # Filter out empty categories
    valid_categories = {
        cat: data for cat, data in results_dict.items()
        if len(data['preds']) > 0 and len(data['gt']) > 0
    }

    if not valid_categories:
        print("No valid categories with predictions found!")
        return

    category_names = sorted(valid_categories.keys())

    # -------------------------------------------------------------
    # Normalized Absolute Error (NAE)
    def compute_nae(preds, gts):
        preds = np.array(preds)
        gts = np.array(gts)
        if len(preds) == 0:
            return np.nan
        return np.mean(np.abs(preds - gts) / gts)

    # -------------------------------------------------------------
    # Compute overall category-wise accuracies for summary bar plot
    summary_accs = []
    for cat in category_names:
        preds = np.array(valid_categories[cat]['preds'])
        gts = np.array(valid_categories[cat]['gt'])
        summary_accs.append((preds == gts).mean())

    # -------------------------------------------------------------
    # Plot summary bar plot of all categories
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.bar(category_names, summary_accs)
    ax.set_ylim(0, 1)
    ax.set_ylabel("Accuracy", fontsize=12)
    ax.set_title(
        f"Accuracy per Object Category â€” {title}"
        if title else "Accuracy per Object Category"
    )
    ax.set_xticks(range(len(category_names)))
    ax.set_xticklabels(category_names, rotation=45, ha='right', fontsize=12)
    ax.grid(True, axis='y', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.show()

    # -------------------------------------------------------------
    # Per-category plots and metrics
    for cat in category_names:
        preds = np.array(valid_categories[cat]['preds'])
        gts = np.array(valid_categories[cat]['gt'])

        # Accuracies
        acc = (preds == gts).mean()
        tol_acc = (np.abs(preds - gts) <= 1).mean()
        nae = compute_nae(preds, gts)

        print(f"\nðŸ“Š Metrics for category: {cat}")
        print(f"   âž¤ Accuracy:          {acc:.4f}")
        print(f"   âž¤ Tolerant Accuracy: {tol_acc:.4f}")
        print(f"   âž¤ NAE:               {nae:.4f}")

        # ---------------------------------------------------------
        # Numerosity-based metrics
        unique_counts = np.unique(gts)
        numerosity_accs = [
            (preds[gts == n] == n).mean() for n in unique_counts
        ]
        tolerant_accs = [
            (np.abs(preds[gts == n] - n) <= 1).mean() for n in unique_counts
        ]

        # ---------------------------------------------------------
        # Plot Accuracy
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.bar(["Exact", "Tolerant"], [acc, tol_acc])
        ax.set_ylim(0, 1)
        ax.set_title(
            f"Accuracy â€” {cat} â€” {title}"
            if title else f"Accuracy â€” {cat}"
        )
        plt.tight_layout()
        plt.show()

        # ---------------------------------------------------------
        # Plot Numerosity Accuracy
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.bar(unique_counts, numerosity_accs)
        ax.set_xticks(unique_counts)
        ax.set_xlabel("Numerosity")
        ax.set_ylabel("Accuracy")
        ax.set_title(f"Numerosity Accuracy â€” {cat}")
        plt.tight_layout()
        plt.show()

        # ---------------------------------------------------------
        # Plot Tolerant Numerosity Accuracy
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.bar(unique_counts, tolerant_accs)
        ax.set_xticks(unique_counts)
        ax.set_xlabel("Numerosity")
        ax.set_ylabel("Tolerant Accuracy (Â±1)")
        ax.set_title(f"Tolerant Numerosity Accuracy â€” {cat}")
        plt.tight_layout()
        plt.show()

        # ---------------------------------------------------------
        # Confusion Matrix
        classes = np.unique(np.concatenate([preds, gts]))
        cm = confusion_matrix(gts, preds, labels=classes)

        fig, ax = plt.subplots(figsize=(6, 5))
        disp = ConfusionMatrixDisplay(cm, display_labels=classes)
        disp.plot(cmap="Blues", colorbar=True, ax=ax)
        ax.invert_yaxis()
        ax.set_title(
            f"Confusion Matrix â€” {cat} â€” {title}"
            if title else f"Confusion Matrix â€” {cat}"
        )
        plt.tight_layout()
        plt.show()
def plot_preds_vs_gts(preds, gts, title=None, save_path=None):
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

    preds = np.array(preds)
    gts = np.array(gts)
    classes = np.arange(1, 13)

    # --- Standard per-class accuracy ---
    accuracies = []
    for cls in classes:
        mask = gts == cls
        acc = (preds[mask] == gts[mask]).mean() if mask.sum() > 0 else np.nan
        accuracies.append(acc)
    avg_acc = np.nanmean(accuracies)

    # --- Tolerant accuracy (Â±1 off is okay) ---
    tolerant_accuracies = []
    for cls in classes:
        mask = gts == cls
        tol_acc = (np.abs(preds[mask] - gts[mask]) <= 1).mean() if mask.sum() > 0 else np.nan
        tolerant_accuracies.append(tol_acc)
    avg_tol_acc = np.nanmean(tolerant_accuracies)

    # --- Compute Numerosity-Aware Error (NAE) per class ---
    def compute_nae(preds, gts):
        preds = np.array(preds)
        gts = np.array(gts)
        if preds.shape != gts.shape:
            raise ValueError("preds and gts must have the same shape")
        weights = 1.0 / gts
        mae_per_sample = np.abs(preds - gts)
        weighted_mae_value = np.mean(mae_per_sample * weights)
        return weighted_mae_value

    nae_per_class = []
    for cls in classes:
        mask = gts == cls
        nae = compute_nae(preds[mask], gts[mask]) if mask.sum() > 0 else np.nan
        nae_per_class.append(nae)
    avg_nae = np.nanmean(nae_per_class)

    # --- Bar plot: Standard accuracy ---
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.bar(classes, accuracies, color="skyblue")
    ax.set_xticks(classes)
    ax.set_ylim(0, 1)
    ax.set_xlabel("Class")
    ax.set_ylabel("Accuracy")
    bar_title = f"Per-Class Accuracy"
    if title:
        bar_title += f" â€” {title}"
    ax.set_title(bar_title)
    ax.grid(True, linestyle="--", alpha=0.6)
    ax.text(0.5, 0.9, f"Avg Acc: {avg_acc:.2f}", transform=ax.transAxes,
            fontsize=12, bbox=dict(facecolor="white", alpha=0.7, edgecolor="black"))
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path.replace(".png", "_bar.png"), bbox_inches="tight", dpi=300)
        plt.close(fig)
    else:
        plt.show()

    # --- Bar plot: Tolerant accuracy (Â±1) ---
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.bar(classes, tolerant_accuracies, color="lightgreen")
    ax.set_xticks(classes)
    ax.set_ylim(0, 1)
    ax.set_xlabel("Class")
    ax.set_ylabel("Tolerant Accuracy (Â±1)")
    tol_title = f"Tolerant Per-Class Accuracy (Â±1)"
    if title:
        tol_title += f" â€” {title}"
    ax.set_title(tol_title)
    ax.grid(True, linestyle="--", alpha=0.6)
    ax.text(0.5, 0.9, f"Avg Tol Acc: {avg_tol_acc:.2f}", transform=ax.transAxes,
            fontsize=12, bbox=dict(facecolor="white", alpha=0.7, edgecolor="black"))
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path.replace(".png", "_tolerant_bar.png"), bbox_inches="tight", dpi=300)
        plt.close(fig)
    else:
        plt.show()

    # --- Bar plot: Numerosity-Aware Error (NAE) ---
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.bar(classes, nae_per_class, color="salmon")
    ax.set_xticks(classes)
    ax.set_xlabel("Class")
    ax.set_ylabel("NAE")
    nae_title = f"Per-Class Numerosity-Aware Error"
    if title:
        nae_title += f" â€” {title}"
    ax.set_title(nae_title)
    ax.grid(True, linestyle="--", alpha=0.6)
    ax.text(0.5, 0.9, f"Avg NAE: {avg_nae:.3f}", transform=ax.transAxes,
            fontsize=12, bbox=dict(facecolor="white", alpha=0.7, edgecolor="black"))
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path.replace(".png", "_nae_bar.png"), bbox_inches="tight", dpi=300)
        plt.close(fig)
    else:
        plt.show()

    # --- Confusion Matrix ---
    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
    cm = confusion_matrix(gts, preds, labels=classes)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
    fig, ax = plt.subplots(figsize=(6, 5))
    disp.plot(ax=ax, cmap="Blues", xticks_rotation=45, colorbar=True)
    ax.invert_yaxis()
    cm_title = "Confusion Matrix"
    if title:
        cm_title += f" â€” {title}"
    ax.set_title(cm_title)
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path.replace(".png", "_cm.png"), bbox_inches="tight", dpi=300)
        plt.close(fig)
    else:
        plt.show()

        
        
def compute_nae(preds, gts):
 
    preds = np.array(preds)
    gts = np.array(gts)
    
    if preds.shape != gts.shape:
        raise ValueError("preds and gts must have the same shape")
    
    # weights = 1 / gts
    weights = 1.0 / gts
    mae_per_sample = np.abs(preds - gts)
    
    weighted_mae_value = np.mean(mae_per_sample * weights)
    return weighted_mae_value
    
    
    
def sample_query_support(dataset, class_names, k_support=5, k_query=15, seed=42):
    """
    Few-shot support/query sampler based on class *names* or *IDs*.
    """
    random.seed(seed)
    indices_per_class = {cls: [] for cls in class_names}

    for idx, (_, label, *_) in enumerate(dataset):
        if label in class_names or str(label) in class_names:
            indices_per_class[label].append(idx)

    support_indices, query_indices = [], []
    for cls in class_names:
        cls_indices = indices_per_class[cls]
        if len(cls_indices) < k_support + k_query:
            print(f"âš ï¸ Class '{cls}' has only {len(cls_indices)} samples, reducing query set.")
        random.shuffle(cls_indices)
        support_indices.extend(cls_indices[:k_support])
        query_indices.extend(cls_indices[k_support:k_support + k_query])

    return Subset(dataset, support_indices), Subset(dataset, query_indices)


def few_shot_nonparametric_eval(
    pefted_vit_with_classifier,
    dataset,
    class_name_list,
    k_support=5,
    k_query=15,
    device="cuda",
    batch_size=16,
    mixed_prec=False,
    tolerance=1
):
    """
    Few-shot non-parametric evaluation using cosine similarity between
    support and query features (no training).

    Returns per-class accuracies, average accuracy, and tolerant accuracy.

    Supports:
      - single int for k_support â†’ standard behavior
      - list of ints for k_support â†’ runs evaluation for each (multi-shot mode)
    """

    # === MULTI-SHOT MODE ===
    if isinstance(k_support, (list, tuple)):
        all_results = []
        for k in k_support:
            print(f"\n{'='*50}\nðŸ”¹ Running evaluation for {k}-shot setting\n{'='*50}")
            result = few_shot_nonparametric_eval(
                pefted_vit_with_classifier,
                dataset,
                class_name_list,
                k_support=k,
                k_query=k_query,
                device=device,
                batch_size=batch_size,
                mixed_prec=mixed_prec,
                tolerance=tolerance
            )
            result["shots"] = k
            all_results.append(result)
        return all_results

    # === SINGLE-SHOT MODE ===
    print(f"\nðŸš€ Starting Non-Parametric Few-Shot Evaluation ({k_support}-shot)")

    # --- Split dataset into support/query subsets ---
    support_ds, query_ds = sample_query_support(dataset, class_name_list, k_support, k_query)
    support_loader = DataLoader(support_ds, batch_size=batch_size, shuffle=False)
    query_loader   = DataLoader(query_ds,   batch_size=batch_size, shuffle=False)

    # --- Extractor (frozen backbone) ---
    vit = deepcopy(pefted_vit_with_classifier.vit).to(device)
    vit.eval()

    def extract_features(dataloader):
        feats, labels = [], []
        with torch.no_grad(), torch.amp.autocast("cuda", enabled=mixed_prec):
            for batch in dataloader:
                images, labels_batch, *_ = batch
                images = images.to(device)
                outputs = vit(images)
                features = outputs.last_hidden_state[:, 0, :]  # CLS token
                features = features / (features.norm(dim=1, keepdim=True) + 1e-8)
                feats.append(features.cpu())
                labels.append(labels_batch)
        return torch.cat(feats), torch.cat(labels)

    # --- Feature extraction ---
    print("ðŸ’¾ Extracting frozen features...")
    support_feats, support_labels = extract_features(support_loader)
    query_feats,   query_labels   = extract_features(query_loader)

    # --- Label remapping (0..N-1) ---
    unique_labels = torch.unique(support_labels)
    label_to_idx = {old.item(): new for new, old in enumerate(unique_labels)}
    support_labels_mapped = torch.tensor([label_to_idx[x.item()] for x in support_labels])
    query_labels_mapped   = torch.tensor([label_to_idx[x.item()] for x in query_labels])
    num_classes = len(label_to_idx)
    feat_dim = support_feats.shape[1]

    print(f"ðŸ§­ {num_classes} classes, feature dim = {feat_dim}")
    print(f"Support set: {len(support_feats)} samples | Query set: {len(query_feats)} samples")

    # --- Compute class prototypes ---
    print("ðŸ§  Computing class prototypes...")
    prototypes = []
    for c in range(num_classes):
        class_feats = support_feats[support_labels_mapped == c]
        proto = class_feats.mean(dim=0)
        proto = proto / (proto.norm() + 1e-8)
        prototypes.append(proto)
    prototypes = torch.stack(prototypes).to(device)

    # --- Evaluate on query set ---
    print("ðŸ” Evaluating query samples...")
    with torch.no_grad():
        query_norm = query_feats / (query_feats.norm(dim=1, keepdim=True) + 1e-8)
        sims = query_norm.to(device) @ prototypes.T
        preds = sims.argmax(dim=1).cpu()

    # --- Compute accuracies ---
    correct = (preds == query_labels_mapped).sum().item()
    total = len(query_labels_mapped)
    avg_acc = correct / total

    # tolerant accuracy (prediction within Â±tolerance)
    tol_correct = (torch.abs(preds - query_labels_mapped) <= tolerance).sum().item()
    avg_tol_acc = tol_correct / total

    # per-class accuracy
    per_class_acc, per_class_tol_acc = [], []
    for c in range(num_classes):
        mask = query_labels_mapped == c
        if mask.sum() == 0:
            per_class_acc.append(float('nan'))
            per_class_tol_acc.append(float('nan'))
            continue
        preds_c = preds[mask]
        labels_c = query_labels_mapped[mask]
        per_class_acc.append((preds_c == labels_c).float().mean().item())
        per_class_tol_acc.append((torch.abs(preds_c - labels_c) <= tolerance).float().mean().item())

    print(f"âœ… Final Non-Parametric Accuracy ({k_support}-shot): {avg_acc*100:.2f}%")
    print(f"ðŸ¤ Tolerant Accuracy (Â±{tolerance}): {avg_tol_acc*100:.2f}%")

    return {
        "shots": k_support,
        "prototypes": prototypes.cpu(),
        "support_feats": support_feats,
        "query_feats": query_feats,
        "support_labels": support_labels_mapped,
        "query_labels": query_labels_mapped,
        "preds": preds,
        "avg_acc": avg_acc,
        "avg_tol_acc": avg_tol_acc,
        "per_class_acc": per_class_acc,
        "per_class_tol_acc": per_class_tol_acc
    }
    
def few_shot_linear_eval(
    pefted_vit_with_classifier,
    dataset,
    class_name_list,
    k_support=5,
    k_query=15,
    device="cuda",
    lr=1e-3,
    epochs=15,
    batch_size=16,
    mixed_prec=False,
    tolerance=1
):
    """
    Few-shot linear classifier evaluation using cached frozen features.

    Returns:
        - avg_acc: average query accuracy
        - avg_tol_acc: tolerant accuracy (Â±tolerance)
        - per_class_acc: per-class accuracy
        - per_class_tol_acc: per-class tolerant accuracy
        - classifier, features, labels, etc.
    
    Supports:
        - int k_support â†’ single evaluation
        - list/tuple of ints â†’ multi-shot evaluation
    """

    # === MULTI-SHOT MODE ===
    if isinstance(k_support, (list, tuple)):
        all_results = []
        for k in k_support:
            print(f"\n{'='*50}\nðŸ”¹ Running linear eval for {k}-shot setting\n{'='*50}")
            result = few_shot_linear_eval(
                pefted_vit_with_classifier,
                dataset,
                class_name_list,
                k_support=k,
                k_query=k_query,
                device=device,
                lr=lr,
                epochs=epochs,
                batch_size=batch_size,
                mixed_prec=mixed_prec,
                tolerance=tolerance
            )
            result["shots"] = k
            all_results.append(result)
        return all_results

    # === SINGLE-SHOT MODE ===
    print(f"\nðŸš€ Starting Few-Shot Linear Evaluation ({k_support}-shot)")

    # --- Split dataset into support/query subsets ---
    support_ds, query_ds = sample_query_support(dataset, class_name_list, k_support, k_query)
    support_loader = DataLoader(support_ds, batch_size=batch_size, shuffle=True)
    query_loader   = DataLoader(query_ds,   batch_size=batch_size, shuffle=False)

    # --- Extractor (frozen backbone) ---
    vit = deepcopy(pefted_vit_with_classifier.vit).to(device)
    vit.eval()

    def extract_features(dataloader):
        feats, labels = [], []
        with torch.no_grad(), torch.amp.autocast("cuda", enabled=mixed_prec):
            for batch in dataloader:
                images, labels_batch, *_ = batch
                images = images.to(device)
                outputs = vit(images)
                features = outputs.last_hidden_state[:, 0, :]   # CLS token
                features = features / (features.norm(dim=1, keepdim=True) + 1e-8)
                feats.append(features.cpu())
                labels.append(labels_batch)
        return torch.cat(feats), torch.cat(labels)

    # --- Feature caching ---
    print("ðŸ’¾ Extracting frozen features...")
    support_feats, support_labels = extract_features(support_loader)
    query_feats,   query_labels   = extract_features(query_loader)

    # --- Label remapping (0..N-1) ---
    unique_labels = torch.unique(support_labels)
    label_to_idx = {old.item(): new for new, old in enumerate(unique_labels)}
    support_labels_mapped = torch.tensor([label_to_idx[x.item()] for x in support_labels])
    query_labels_mapped   = torch.tensor([label_to_idx[x.item()] for x in query_labels])
    num_classes = len(label_to_idx)
    feat_dim = support_feats.shape[1]

    print(f"ðŸ§­ {num_classes} classes, feature dim = {feat_dim}")
    print(f"Support set: {len(support_feats)} samples | Query set: {len(query_feats)} samples")

    # --- Linear classifier ---
    classifier = nn.Linear(feat_dim, num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(classifier.parameters(), lr=lr)

    # --- Train linear head ---
    print("âš™ï¸ Training linear classifier on cached features...")
    for epoch in range(epochs):
        classifier.train()
        total_loss, correct, total = 0, 0, 0
        for i in range(0, len(support_feats), batch_size):
            feats = support_feats[i:i+batch_size].to(device)
            labels = support_labels_mapped[i:i+batch_size].to(device)

            logits = classifier(feats)
            loss = criterion(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item() * feats.size(0)
            correct += (logits.argmax(1) == labels).sum().item()
            total += labels.size(0)

    # --- Evaluate on query set ---
    classifier.eval()
    preds_list = []
    with torch.no_grad():
        for i in range(0, len(query_feats), batch_size):
            feats = query_feats[i:i+batch_size].to(device)
            labels = query_labels_mapped[i:i+batch_size].to(device)
            logits = classifier(feats)
            preds = logits.argmax(1).cpu()
            preds_list.append(preds)
    preds = torch.cat(preds_list)

    # --- Compute metrics ---
    avg_acc = (preds == query_labels_mapped).float().mean().item()
    avg_tol_acc = (torch.abs(preds - query_labels_mapped) <= tolerance).float().mean().item()

    per_class_acc, per_class_tol_acc = [], []
    for c in range(num_classes):
        mask = query_labels_mapped == c
        if mask.sum() == 0:
            per_class_acc.append(float('nan'))
            per_class_tol_acc.append(float('nan'))
            continue
        preds_c = preds[mask]
        labels_c = query_labels_mapped[mask]
        per_class_acc.append((preds_c == labels_c).float().mean().item())
        per_class_tol_acc.append((torch.abs(preds_c - labels_c) <= tolerance).float().mean().item())

    print(f"âœ… Final Few-Shot Accuracy ({k_support}-shot): {avg_acc*100:.2f}%")
    print(f"ðŸ¤ Tolerant Accuracy (Â±{tolerance}): {avg_tol_acc*100:.2f}%")

    return {
        "shots": k_support,
        "classifier": classifier,
        "support_feats": support_feats,
        "query_feats": query_feats,
        "support_labels": support_labels_mapped,
        "query_labels": query_labels_mapped,
        "preds": preds,
        "avg_acc": avg_acc,
        "avg_tol_acc": avg_tol_acc,
        "per_class_acc": per_class_acc,
        "per_class_tol_acc": per_class_tol_acc
    }
    
    


def evaluate_few_shot_per_object_type(pefted_vit, datasets_dict, object_types, shots_list=[1,5,10],
                                      k_query=15, device="cuda", lr=1e-3, epochs=15, batch_size=16,
                                      mixed_prec=False, tolerance=1, title=None, save_path=None):

    results_summary = {}
    cmap = plt.get_cmap("tab10")
    colors = [cmap(i % 10) for i in range(len(object_types))]

    plt.figure(figsize=(10,6))

    for idx, obj_type in enumerate(object_types):
        dataset = datasets_dict[idx]
        results_summary[obj_type] = {'avg_acc': [], 'avg_tol_acc': []}

        cat_results = []
        for k in shots_list:
            result = few_shot_linear_eval(
                pefted_vit,
                dataset,
                class_name_list=[0,1,2,3,4,5,6,7,8,9],
                k_support=k,
                k_query=k_query,
                device=device,
                lr=lr,
                epochs=epochs,
                batch_size=batch_size,
                mixed_prec=mixed_prec,
                tolerance=tolerance
            )
            cat_results.append(result)

            # Store avg_acc and avg_tol_acc per shot
            results_summary[obj_type]['avg_acc'].append(result['avg_acc'])
            results_summary[obj_type]['avg_tol_acc'].append(result['avg_tol_acc'])

        # Plot object type: average accuracy across shots
        plt.plot(shots_list, results_summary[obj_type]['avg_acc'], marker='o',
                 color=colors[idx], label=obj_type)

    plt.xticks(shots_list, fontsize=14)      # bigger x-ticks
    plt.yticks(fontsize=14)                  # bigger y-ticks
    plt.xlabel("Shots (k_support)", fontsize=16)
    plt.ylabel("Few-Shot Accuracy", fontsize=16)
    plt.title(
      f"Few-Shot Accuracy per Object Type â€” {title}" if title else "Few-Shot Accuracy per Object Type",
      fontsize=18)


    plt.grid(True, linestyle='--', alpha=0.5)
    plt.legend(fontsize=12)
    plt.tight_layout()

    if save_path:
     plt.savefig(save_path)

    plt.show()

    return results_summary

def compare_two_models_few_shot(pefted_vit_base, pefted_vit_tuned, datasets_dict, object_types,
                                shots_list=[1,2,4,8,16], k_query=32, device="cuda",
                                lr=1e-3, epochs=50, batch_size=16,
                                mixed_prec=True, tolerance=1, title=None, save_path=None):

    results_avg = { "Base": [], "Tuned": [] }

    for k in shots_list:
        avg_acc_base = []
        avg_acc_tuned = []

        for ix,obj_type in enumerate(object_types):
            dataset = datasets_dict[ix]

            res_base = few_shot_linear_eval(
                pefted_vit_base, dataset, class_name_list=list(range(10)),
                k_support=k, k_query=k_query, device=device,
                lr=lr, epochs=epochs, batch_size=batch_size,
                mixed_prec=mixed_prec, tolerance=tolerance
            )
            avg_acc_base.append(res_base['avg_acc'])

            res_tuned = few_shot_linear_eval(
                pefted_vit_tuned, dataset, class_name_list=list(range(10)),
                k_support=k, k_query=k_query, device=device,
                lr=lr, epochs=epochs, batch_size=batch_size,
                mixed_prec=mixed_prec, tolerance=tolerance
            )
            avg_acc_tuned.append(res_tuned['avg_acc'])

        results_avg["Base"].append(np.mean(avg_acc_base))
        results_avg["Tuned"].append(np.mean(avg_acc_tuned))

    plt.figure(figsize=(10,6))
    plt.plot(shots_list, results_avg["Base"], marker='o', label="Base")
    plt.plot(shots_list, results_avg["Tuned"], marker='s', label="Tuned")
    plt.xticks(shots_list)
    plt.xlabel("Shots (k_support)")
    plt.ylabel("Average Few-Shot Accuracy")
    plt.title(f"Average Few-Shot Accuracy Comparison â€” {title}" if title else "Few-Shot Accuracy Comparison")
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.legend()
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path)
    plt.show()

    return results_avg

def plot_class_metrics(results_dict, class_names=None, mode=None, show_plots=True):
    """
    Compute and plot classification metrics.

    Modes:
      â€¢ Single dict  â†’ per-class metrics + plots
      â€¢ List of dicts (each has 'shots') â†’ Accuracy vs Shots plots only
    """
    # === Case 1: Multiple dicts (few-shot comparison) ===
    if isinstance(results_dict, list):
        shots_list, acc_list, tol_acc_list = [], [], []

        for i, res in enumerate(results_dict):
            if "shots" not in res:
                raise ValueError(f"Missing 'shots' key in results_dict[{i}]")

            # Disable per-class plots for inner calls
            metrics = plot_class_metrics(res, class_names=class_names, mode=mode, show_plots=False)
            shots_list.append(res["shots"])
            acc_list.append(metrics["avg_acc"])
            tol_acc_list.append(metrics["avg_tol_acc"])

        # Sort results by number of shots
        shots, accs, tol_accs = zip(*sorted(zip(shots_list, acc_list, tol_acc_list)))

        # Plot performance vs shots
        plt.figure(figsize=(8, 5))
        plt.plot(shots, accs, "-o", label="Average Accuracy", color="skyblue", linewidth=2)
        plt.plot(shots, tol_accs, "-o", label="Tolerant Accuracy", color="lightgreen", linewidth=2)
        plt.title("Performance vs. Shots")
        plt.xlabel("Number of Shots")
        plt.ylabel("Accuracy")
        plt.legend()
        plt.grid(True, linestyle="--", alpha=0.6)
        plt.tight_layout()
        plt.show()

        return {
            "shots": list(shots),
            "avg_acc": list(accs),
            "avg_tol_acc": list(tol_accs),
        }

    # === Case 2: Single dict (base behavior) ===
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # --- Determine mode automatically if not provided ---
    if mode is None:
        if "classifier" in results_dict:
            mode = "parametric"
        elif "prototypes" in results_dict:
            mode = "nonparametric"
        else:
            raise ValueError("results_dict must contain either 'classifier' or 'prototypes'.")

    query_feats = results_dict["query_feats"].to(device)
    query_labels = results_dict["query_labels"].to(device)

    # --- Predict ---
    with torch.no_grad():
        if mode == "parametric":
            classifier = results_dict["classifier"].to(device)
            logits = classifier(query_feats)
            preds = logits.argmax(1)
        elif mode == "nonparametric":
            prototypes = results_dict["prototypes"].to(device)
            query_norm = query_feats / (query_feats.norm(dim=1, keepdim=True) + 1e-8)
            sims = query_norm @ prototypes.T
            preds = sims.argmax(1)
        else:
            raise ValueError(f"Unknown mode '{mode}'")

    # --- Compute metrics ---
    labels_np = query_labels.cpu().numpy()
    preds_np = preds.cpu().numpy()
    n_classes = len(np.unique(labels_np))
    class_names = class_names or [f"Class {i}" for i in range(n_classes)]

    accuracies, mae, tolerant_acc = [], [], []
    for c in range(n_classes):
        mask = labels_np == c
        cls_preds = preds_np[mask]
        cls_labels = labels_np[mask]
        if len(cls_preds) == 0:
            accuracies.append(np.nan)
            mae.append(np.nan)
            tolerant_acc.append(np.nan)
            continue
        acc = (cls_preds == cls_labels).mean()
        err = np.abs(cls_preds - cls_labels).mean()
        tol_acc = (np.abs(cls_preds - cls_labels) <= 1).mean()
        accuracies.append(acc)
        mae.append(err)
        tolerant_acc.append(tol_acc)

    # --- Compute global averages ---
    avg_acc = np.nanmean(accuracies)
    avg_mae = np.nanmean(mae)
    avg_tol_acc = np.nanmean(tolerant_acc)

    # --- Print summary ---
    print(f"\nðŸ“Š Metrics Summary ({mode}):")
    print(f"   âž¤ Average Accuracy:          {avg_acc:.4f}")
    print(f"   âž¤ Average MAE:               {avg_mae:.4f}")
    print(f"   âž¤ Average Tolerant Accuracy: {avg_tol_acc:.4f}")

    # --- Plot per-class metrics if enabled ---
    if show_plots:
        plt.figure(figsize=(15, 4))

        plt.subplot(1, 3, 1)
        plt.bar(range(n_classes), accuracies, color="skyblue")
        plt.xticks(range(n_classes), class_names, rotation=45, ha="right")
        plt.title(f"Per-Class Accuracy ({mode})")
        plt.ylabel("Accuracy")

        plt.subplot(1, 3, 2)
        plt.bar(range(n_classes), mae, color="salmon")
        plt.xticks(range(n_classes), class_names, rotation=45, ha="right")
        plt.title(f"Per-Class MAE ({mode})")
        plt.ylabel("MAE")

        plt.subplot(1, 3, 3)
        plt.bar(range(n_classes), tolerant_acc, color="lightgreen")
        plt.xticks(range(n_classes), class_names, rotation=45, ha="right")
        plt.title(f"Per-Class Tolerant Accuracy ({mode})")
        plt.ylabel("Tolerant Accuracy (|pred - label| â‰¤ 1)")

        plt.tight_layout()
        plt.show()

    return {
        "mode": mode,
        "per_class_acc": accuracies,
        "per_class_mae": mae,
        "per_class_tol_acc": tolerant_acc,
        "avg_acc": avg_acc,
        "avg_mae": avg_mae,
        "avg_tol_acc": avg_tol_acc,
        "preds": preds_np,
        "labels": labels_np,
    }
    
    
def compare_baseline_tuned_metrics(baseline_results, tuned_results, shots_list=None, mode="nonparametric"):
   
    # --- Extract metrics ---
    baseline_acc = [r.get("avg_acc", r.get("acc", np.nan)) for r in baseline_results]
    tuned_acc    = [r.get("avg_acc", r.get("acc", np.nan)) for r in tuned_results]

    baseline_tol_acc = [r.get("avg_tol_acc", np.nan) for r in baseline_results]
    tuned_tol_acc    = [r.get("avg_tol_acc", np.nan) for r in tuned_results]

    # Extract shot counts automatically if not provided
    if shots_list is None:
        shots_list = [r.get("shots", i+1) for i, r in enumerate(baseline_results)]

    # --- Plot Accuracy Comparison ---
    plt.figure(figsize=(7, 5))
    plt.plot(shots_list, baseline_acc, marker='o', linestyle='-', color='skyblue', label='Baseline')
    plt.plot(shots_list, tuned_acc, marker='s', linestyle='--', color='orange', label='Tuned')
    plt.title(f"Few-Shot Accuracy Comparison ({mode})")
    plt.xlabel("Shots per Class")
    plt.ylabel("Accuracy")
    plt.xticks(shots_list)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # --- Plot Tolerant Accuracy Comparison ---
    plt.figure(figsize=(7, 5))
    plt.plot(shots_list, baseline_tol_acc, marker='o', linestyle='-', color='skyblue', label='Baseline')
    plt.plot(shots_list, tuned_tol_acc, marker='s', linestyle='--', color='orange', label='Tuned')
    plt.title(f"Tolerant Accuracy Comparison ({mode})")
    plt.xlabel("Shots per Class")
    plt.ylabel("Tolerant Accuracy (|pred - label| â‰¤ 1)")
    plt.xticks(shots_list)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.show()